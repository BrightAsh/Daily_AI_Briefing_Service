import os
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.agents import Tool, initialize_agent, AgentType
from langchain.chat_models import ChatOpenAI
import requests
import gradio as gr

# ğŸ” SerpAPI ëŒ€ì²´ GoogleSearch í´ë˜ìŠ¤
class GoogleSearch:
    def __init__(self, params):
        self.params = params
        self.api_key = params.get("api_key")

    def get_dict(self):
        response = requests.get("https://serpapi.com/search", params=self.params)
        response.raise_for_status()
        return response.json()

# ğŸ” í™˜ê²½ ë³€ìˆ˜
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

# ğŸ“š ë²¡í„°DB ë¡œë”©
base_path = os.path.dirname(__file__)
CHUNK_PATH = os.path.join(base_path, "faiss_index", "doc_chunks.npy")
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
chunks = np.load(CHUNK_PATH, allow_pickle=True)
embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
documents = [
    Document(page_content=c["text"], metadata={"source": c["source"]})
    for c in chunks if isinstance(c, dict)
]
vectordb = FAISS.from_documents(documents, embedding=embedding_model)

# ğŸ” RAG í•¨ìˆ˜
def filtered_rag_run(query: str, source_filter: str) -> str:
    filtered_docs = [doc for doc in documents if doc.metadata.get("source") == source_filter]
    if not filtered_docs:
        return f"âš ï¸ '{source_filter}' ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    local_vectordb = FAISS.from_documents(filtered_docs, embedding=embedding_model)
    local_retriever = local_vectordb.as_retriever(search_type="mmr", search_kwargs={"k": 3})
    rag_chain = RetrievalQA.from_chain_type(
        retriever=local_retriever,
        llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3),
        return_source_documents=False
    )
    docs = local_retriever.get_relevant_documents(query)
    if not docs or all(len(doc.page_content.strip()) < 50 for doc in docs):
        return f"ğŸ” '{source_filter}' ë¬¸ì„œì—ì„œ ìœ íš¨í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    return rag_chain.run(query)

# âœ¨ ìš”ì•½ íˆ´
def run_summary_tool(text: str) -> str:
    messages = [
        {"role": "system", "content": (
            "ë„ˆëŠ” AI ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì•¼. Final AnswerëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´. "
            "ë²¡í„° DBì˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë©´ ì§ì ‘ ë‹µí•˜ê±°ë‚˜ ì ì ˆí•œ ìš”ì•½ì„ ì œê³µí•´ì¤˜. "
        )},
        {"role": "user", "content": f"{text}"}
    ]
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0.1
    )
    return response.choices[0].message.content

# ğŸŒ ì›¹ ê²€ìƒ‰ íˆ´
def search_web_tool(query: str) -> str:
    search = GoogleSearch({
        "q": query,
        "api_key": SERPAPI_API_KEY,
        "num": 3
    })
    results = search.get_dict()

    if "organic_results" not in results:
        return "âŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    output = "ğŸŒ ì•„ë˜ ì •ë³´ëŠ” Google ê²€ìƒ‰(SerpAPI) ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤:\n"
    for item in results["organic_results"][:3]:
        title = item.get("title", "ì œëª© ì—†ìŒ")
        link = item.get("link", "")
        snippet = item.get("snippet", "")
        output += f"\nğŸ”— [{title}]({link})\n{snippet}\n"
    return output

# ğŸ› ï¸ Tool ì •ì˜
tools = [
    Tool("news_query_tool", lambda q: filtered_rag_run(q, "news"), "ë‰´ìŠ¤ ê´€ë ¨ ì§ˆì˜"),
    Tool("blog_query_tool", lambda q: filtered_rag_run(q, "blog"), "ë¸”ë¡œê·¸ ê´€ë ¨ ì§ˆì˜"),
    Tool("paper_query_tool", lambda q: filtered_rag_run(q, "paper"), "ë…¼ë¬¸ ê´€ë ¨ ì§ˆì˜"),
    Tool("text_summarizer", run_summary_tool, "ë¬¸ë§¥ ìš”ì•½"),
    Tool("web_search", search_web_tool, "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰")
]

# ğŸ§  ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
agent = initialize_agent(
    tools=tools,
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3),
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    handle_parsing_errors=True,
    verbose=True
)

# ğŸ’¬ Gradio ì±— UI ì •ì˜
def chat_with_agent(message, history):
    try:
        response = agent.run(message)
    except Exception as e:
        response = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    history.append((message, response))
    return "", history

# ğŸ–¼ï¸ Gradio ì¸í„°í˜ì´ìŠ¤
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§  Daily AI Briefing Chatbot")

    chatbot = gr.Chatbot(label="ğŸ§  AI ì±—ë´‡")
    msg = gr.Textbox(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!", label="ğŸ’¬ ì…ë ¥")
    clear = gr.Button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”")

    msg.submit(chat_with_agent, [msg, chatbot], [msg, chatbot])
    clear.click(lambda: [], None, chatbot)

if __name__ == "__main__":
    demo.launch(share=True)
