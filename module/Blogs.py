import sys
sys.path.append("E:\Daily_AI_Briefing_Service")

from function_dev.synonym_finder import find_synonyms
from function_dev.web_crawler import crawl_tistory_blogs_google
from function_dev.blog_summarizer import hierarchical_summary, is_relevant

def Blogs_pipeline(keyword, days, n=1, country='Korea'):
    keywords = find_synonyms(keyword, n, country)
    summarized_blogs = []
    seen_urls = set()   # âœ… ì¤‘ë³µ ë°©ì§€ìš©

    for keyword in keywords:
        blogs = crawl_tistory_blogs_google(keyword, days, 20)
        for idx, blog in enumerate(blogs, 1):
            title = blog.get("title", "")
            full_text = blog.get("full_text", "").strip()
            url = blog.get("url")

            if not full_text:
                print(f"\n[{idx}] âš ï¸ {title}: ë³¸ë¬¸ ì—†ìŒ (ìŠ¤í‚µ)")
                continue

            # âœ… URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
            if url in seen_urls:
                print(f"\n[{idx}] ğŸš« {title}: ì´ë¯¸ ì²˜ë¦¬ëœ ë¸”ë¡œê·¸ (ì¤‘ë³µ ìŠ¤í‚µ)")
                continue
            seen_urls.add(url)

            print(f"\n[{idx}] ğŸ“° {title}")
            print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(full_text)}ì")

            try:
                summary = hierarchical_summary(full_text, keyword)
                if summary is None:  # âœ… ìš”ì•½ ê²°ê³¼ê°€ Noneì¸ ê²½ìš° ìŠ¤í‚µ
                    print(f"âš ï¸ {title}: í‚¤ì›Œë“œì™€ ë¬´ê´€í•œ ë¸”ë¡œê·¸")
                    continue

                print(f"âœ… ìµœì¢… ìš”ì•½ ì™„ë£Œ:\n{summary[:500]}...")

                summarized_blogs.append({
                    "title": title,
                    "url": url,
                    "summary": summary
                })
            except Exception as e:
                print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
    return summarized_blogs

if __name__ == "__main__":
    Blogs_pipeline("ai", 3)