[
  {
    "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene   Generation",
    "body": "5 2 0 2  y a M 5  ]  V C . s c [  1 v 6 3 8 2 0 . 5 0 5 2 : v i X r a  Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation  Lu Ling1,2, Chen-Hsuan Lin1, Tsung-Yi Lin1, Yifan Ding1, Yu Zeng1, Yichen Sheng1, Yunhao Ge1, Ming-Yu Liu1, Aniket Bera2*, Zhaoshuo Li1*  1NVIDIA Research  2Purdue University  https://research.nvidia.com/labs/dir/scenethesis  Figure 1. Scenethesis is a framework for text to interactive 3D scene generation. Given a text prompt, Scenethesis leverages both language and visual priors to generate realistic and physical plausible indoor and outdoor environments.   Synthesizing interactive 3D scenes from text is crucial for gaming [15], virtual content creation [33], and embodied AI [7, 18, 19, 32, 47, 48]. Instead of generating a sin- gle scene geometry [14] or differentiable rendering prim- itives [49], interactive 3D scene synthesis focuses on arrang- ing individual objects to construct a realistic layout while preserving natural interactions, function roles, and physi- cal principles. For example, chairs should face tables to accommodate seating, and small items are typically placed inside cabinets, drawers, and shelves without penetration. Capturing these spatial relationships is crucial for generat- ing realistic scenes, allowing virtual environments to reflect real-world structure and coherence.  Traditional interactive scene generation methods, includ- ing manual design [13, 18, 21], are often labor intensive and thus unscalable, while procedural approaches [6] pro- duce overly simplified scenes and fail to capture various real-world spatial relations. In recent years, deep learning- based scene generation methods, such as auto-regressive models [34] and diffusion approaches [41, 47], have enabled end-to-end generation of 3D layouts. However, they rely on object-annotated datasets like 3D-FRONT [12], which are small in scale, limited to indoor environments, and often contain collisions [47]. These datasets primarily model large furniture layouts while neglecting smaller objects and their functional interactions.  The emergence of large language models (LLMs) [10, 20, 48] expands scene diversity by leveraging common-sense knowledge from text, such as which objects should co-occur based on human intent. However, their lack of visual percep- tion prevents them from accurately reproducing real-world spatial relations, leading to unrealistic object placements that disregard functional roles, human intent, and physical con- straints. As illustrated in Figure 2, LLM-generated scenes often misorient (e.g., chairs facing the cabinet) and misplace (e.g., cabinet placed against the window) objects; small ob- jects are restricted to predefined locations (e.g., only on top of cabinets instead of inside). This lack of realism disrupts object functionality, weakens spatial coherence, and hinders structural consistency, ultimately making LLM-generated scenes impractical for real-world usability and interactions. Building on insights from vision foundation models that  Figure 2. Unrealistic 3D scenes generated by the LLM-based method (Holodeck [48]), exhibiting misplaced objects and oversim- plified spatial relations.  encode compact spatial information and generate coherent scene distributions reflecting real-world layouts, we intro- duce Scenethesis â€“ a training-free agentic framework that integrates LLM-based scene planning with vision-guided spatial refinement. Building on top of LLMs, which lack real-world perception, Scenethesis enforces vision-based spatial constraints to enhance realism and physical plausi- bility. Given a text prompt, Scenethesis employs an LLM for reasoning of coarse layout, a vision module for layout refinement, depth estimation, structural extraction, and a novel optimization for iterative alignment of object place- ment with visual prior through semantic correspondence matching and signed distance field (SDF)-based physical constraints, ensuring collision-free and stable integration into digital environments. Finally, a judge module verifies the spatial coherence. Quantitative and qualitative results demonstrate that Scenethesis outperforms SOTA methods in scene diversity (generating indoor and outdoor scenes), layout realism, and physical plausibility. The layouts gen- erated from Scenethesis can be used for downstream tasks such as virtual content creation, editing, and simulation. Our contribution is summarized as follows. â€¢ We introduce Scenethesis, a training-free agentic frame- work, integrates LLMs, vision foundation models, physical-aware optimization, and scene judgment to col- laboratively generate realistic 3D interactive scenes.  â€¢ Scenethesis integrates LLMâ€™s common-sense reasoning for coarse scene planning with vision-guided spatial refine- ment, effectively capturing realistic inter-object relations. â€¢ We propose a novel optimization process that iteratively aligns objects using semantic correspondence matching and SDF-based physical constraints, enforcing collision- free, stable, and semantically correct placements.  â€¢ We assess the diversity, layout realism, and object interac- tivity of scenes generated by Scenethesis, demonstrating superior spatial realism and physical plausibility compared to SOTA methods.  2. Related Work  Indoor Scene Synthesis. Realistic indoor scene synthe- sis is essential for simulating interactive environments  2  A relax bar lounge with chairs a wine cellar with red wall bricks and training embodied agents for real-world tasks Early methods framed this task as layout prediction, represent- ing scenes as graphs with object relations [3, 30, 56] or hierarchical structures [23, 42]. SceneFormer [43] and ATISS [34] introduced autoregressive models to infer spatial relations with 3D bounding box supervision. Recent ap- proaches learn layout distributions from 3D datasets like 3D- FRONT [12], while DiffuScene [41] and InstructScene [25] integrate object semantics and geometry into diffusion pro- cesses. PhyScene [47] incorporates physical constraints. However, interactive scene generation methods remain dataset-constrained, limiting generalization and often pro- ducing unrealistic compositions due to relaxed collision con- straints [41, 43]. Instead of learning layout distributions from limited 3D datasets, Scenethesis derives spatial priors from image generation models, enabling broader generalization across diverse scenarios. LLM/VLM Guided 3D Scene Generation. Early ef- forts [6, 7, 37] relied on rule-based procedural mod- eling to define spatial relations for interactive environ- ments. With the rise of LLMs/VLMs, recent methods such as SceneTeller [33], Holodeck [48], SceneCraft [20], GALA3D [55], RobotGen [44], Open-Universe [1], GenUSD [26], LayoutVLM [40] and SceneX [54] leverage LLMs/VLMs for: (1) spatial relation planning via prede- fined implicit relations, (2) 3D asset retrieval from semantic descriptions or vision-language embeddings, and (3) rule- based rough collision detection, demonstrating large-scale scene generation potential. Although LLMs encode rich common sense knowledge, they struggle with fine-grained spatial reasoning. Predefined spatial relations in text descrip- tions are often simplistic, limiting their ability to capture the complexity of the real-world scene [17, 25]. In contrast, Scenethesis leverages LLM priors to convert text prompts into coarse layout instructions while using vision foundation model to persevere compact spatial information, effectively capturing real-world spatial complexity. Visual Foundation Model-Guided Scene Generation. Vi- sual foundation models (VFMs), particularly image gen- eration models, have advanced visual generation and are now widely applied to 3D scene synthesis. Methods such as Text2Room [14], SceneScape [11], WonderJourney [50], WonderWorld [49], and Text2NeRF [52] integrate 2D diffu- sion with 3D priors (e.g., depth) to generate single-geometry scenes. However, this approach inherently faces challenges in handling occlusions and reconstructing hidden elements due to the interconnected structure of real-world scenes, making them unsuitable for object interactions.  Architect [45] and Deep Prior Assembly (DPA) [53] in- troduce 2D inpainting for interactive 3D scene generation and reconstruction. While this improves occlusion handling, the lack of physical constraints and 3D reasoning leads to misaligned, floating, or intersecting objects, making it diffi-  cult to maintain functional object relationships for embodied AI tasks. In contrast, Scenethesis integrates physics-aware optimization, ensuring both spatial alignment with realistic visual prior and physical plausibility. Physics-Aware Scene Generation. Physical principles have been largely overlooked in 3D interactive scene genera- tion for both LLM-based and VFM-based methods. Recent works, such as PhyScene [47] and Holodeck [48] enforce physical constraints by detecting collisions using 3D bound- ing boxes. While PhyScene reduces collision rates, it still exceeds 15% [47]. Holodeck focuses only on large-object collision avoidance, neglecting small-object inter-collisions. Despite these advances, achieving full physical plausibility remains a challenge. To address this, Scenethesis incor- porates precise collision detection and stability constraints, significantly reducing collision and instability rates.  3. Method  Scenethesis generates spatially realistic, physically plau- sible interactive 3D environments from user prompts. An overview of the pipeline is shown in Figure 3, consisting of four key stages: (1) an LLM module drafts a coarse scene plan, (2) a vision module refines the layout with vi- sual guidance and structural extraction, (3) a physical-aware optimization module distills priors and adjusts object place- ment for spatial coherence and physical plausibility, and (4) a scene judge module verifies spatial consistency. The following sections detail each moduleâ€™s role.  3.1. Coarse Scene Planning  Scenethesis supports either a simple prompt (e.g., â€œa peace- ful beach during sunsetâ€) for flexible scene generation or a detailed prompt for controllable scene generation (e.g., a scene plan describing the detailed spatial relations as shown in the appendix). For a simple prompt, the LLM gener- ates a coarse scene plan by reasoning over user input. It first interprets the prompt, reviews all object categories in the available 3D database, selects commonly associated ob- jects, and then generates an up-sampled prompt describing coarse spatial relations, as illustrated in Figure 3. When given detailed prompts, the LLM checks for the presence of all specified objects in the database, infers relevant object categories, and skips the prompt up-sampling process.  Among the selected objects, the LLM identifies an an- chor object, following prior work [48]. The anchor serves as the central reference point, occupying the highest spatial hierarchy apart from the ground. Then the LLM establishes a coarse spatial hierarchy, positioning objects relative to the anchor and incorporating these relationships into the upsam- pled prompt. For example, in a cozy living room, the sofa acts as the anchor at the center, while a bookshelf is placed in the background, aligned against the wall. Other objects,  3   Figure 3. Scenethesis is an agentic framework. The LLM module performs coarse scene planning, estimating rough spatial relationships. The vision module refines this layout by enforcing accurate spatial constraints. The physical-aware optimization iteratively adjusts object placement, ensuring pose alignment and physical plausibility. Finally, a judge module verifies the scene spatial coherence.  such as a coffee table or chairs, are positioned in front of or beside the sofa.  3.2. Layout Visual Refinement  A key insight of Scenethesis is that image generation mod- els inherently encode object functionality and spatial rela- tionships by learning common co-occurrences and spatial arrangements from large-scale image datasets. The vision module refines the coarse layout through three steps: (1) Image Guidance â€“ Generates images to refine spatial rela- tions, ensuring realism and object functionality. (2) Scene Graph Generation â€“ Segments objects, estimates depth and 3D bounding boxes, and constructs a graph encoding inter- object relationships to establish the initial layout. (3) Asset Retrieval â€“ Selects 3D assets and environment maps for final scene composition. Image Generation. The vision module refines the upsam- pled prompt into a visually structured scene representation. This generated image serves as the basis for segmentation, depth estimation, and asset retrieval. Scene Graph Generation. Leveraging vision foundation models such as GPT-4o [16], Grounded-SAM [39], and DepthPro [2], the vision module constructs a scene graph that localizes objects using 3D bounding boxes (3DBB) and identifies structural components, including the anchor object, parent objects, and child objects (see Figure 3).  To initialize asset 5DoF poses, vision module segments objects using semantic cues, estimates depth maps, and projects them into a 3D point cloud. However, due to occlu- sion, limited perspectives, and segmentation errors, cropped image guidance may miss full object visibility, leading to biases in 3DBB estimation â€“ necessitating pose adjustments later (Sec. 3.3.1).  The scene graph forms the basis for iterative 5DoF pose  adjustments during optimization in the next stage. Since Scenethesis focuses on ground-level object layout, back- ground elements, e.g. wall decorations, are visually defined by the retrieved environment map. Detailed scene graph formatting instructions are provided in the appendix. Asset Retrieval. Unlike existing 3D object generation and reconstruction techniques [24, 46], such as 3D Gaussian splatting, which can produce photorealistic visuals but suffer from artifacts and geometric inconsistencies. These methods lack editable meshes, UV mappings, and decomposable PBR materials, making them incompatible with standard produc- tion workflows. To address these limitations, Scenethesis adopts a retrieval-based approach for asset selection, ensur- ing both geometric fidelity and editability for downstream applications. We construct a high-quality asset subset from Objaverse [8] similar to Holodeck [48], and supplemented with a custom environment map dataset. In the final step, the 3D assets and an environment map are retrieved to assemble a visually coherent scene. Retrieval details can be found in appendix.  3.3. Physics-aware Optimization  Directly placing 3D assets based on estimated point clouds from image guidance poses significant challenges: (1) Occlu- sions in real-world scenarios result in incomplete 3D point clouds, leading to errors in object orientation, scale, and po- sition. (2) Discrepancies between retrieved assets and image guidance in texture and shape make precise pose estimation difficult. To overcome these issues, Scenethesis employs a physics-aware optimization powered by robust semantic feature matching [4, 9, 51] and signed-distance fields (SDFs). This optimization process iteratively refines object poses to ensure pose alignment and physical plausibility.  4  User Prompt:â€œA peaceful beach during sunsetâ€VLMGrounded SAMDepth ProLLMInstruction:Reasoning the promptâ€¦Verifying database objectsâ€¦Selecting common objects â€¦Planning a coarse layoutâ€¦LLM Module: Coarse scene planningVision Module: Layout visual refinementOptimization Module: Physics-aware optimizationVLMJudge Module: Spatial coherence judgmentGenerated scenePartial ViewCorrespondenceSDF-based ConstraintObject listâ€¢Beach towelâ€¢Beach chair â€¢Umbrellaâ€¢Volleyball â€¢Seashell â€¢â€¦Upsampled promptThis is a â€¦ beach. The seashell is placed on the beach towelâ€¦ The beach chair is placed in front of â€¦Image guidance2DGeneration Objaverse databaseâ€¦Retrieved assets & environment mapchair2ball1bucket3groundsunglass2sunglass1seashellScene graphumbrella1chair1bucket1bucket2 efficiency, we uniformly sample n points from its triangle surface mesh as its geometric representation and compute its centroid for collision avoidance. Collision Constraints. We query the scene SDFs using object surface points to detect collision states and define po- sition collision loss Ltranslation and scale collision loss Lscale. As shown in Figure 4, the deviation caused by collisions impacts translation T as:  Ltranslation =  (cid:88)  viâˆˆVâˆ’  ||f (T, |di|, ui) âˆ’ T||2 2,  (1)  where f (T, |di|, ui) = T+uiÂ·|di| computes a collision-free position by adjusting the translation along direction ui with step size |di|. Here, di is the negative SDF value at a collided point vi, which belong to the points set with negative SDF Vâˆ’ sampled uniformly from the surface. The direction u is defined from the collision point toward the modelâ€™s centroid, guiding objects away from collisions.  Collisions also affect object scale s due to opposing  forces:  Lscale =  ï£± ï£´ï£²  (cid:80)  viâˆˆVâˆ’  ï£´ï£³  0  (cid:18)  (cid:19)2  g(|di|, ui) âˆ’ s  if Ncluster > 1  ,  otherwise  ||ui||  (2) where g(|di|, ui) = ||ui||âˆ’|di| defines the target scale to re- duce collision regions. Ncluster denotes the number of distinct clusters formed without SDF sign flipping. As shown in Fig- ure 4, two surface points i and j with di â‰¤ 0 and dj â‰¤ 0 belong to different clusters, and thus push the object to be smaller. Stability Constraints. Objects are dragged by gravity and rest on their bottom contacting surface. We ensure stability by enforcing contact between an objectâ€™s bottom points and its parent surface, where their SDF values should be zero, as shown in Figure 4. The stability loss is defined as:  Lstability =  (cid:18)  (cid:88)  viâˆˆVB  (cid:19)  1 âˆ’ exp(âˆ’d2 i )  ,  (3)  where VB are the sampled points on the bottom surface of bounding box, and di are their corresponding SDF values. Further details on collision loss optimization are provided in the Appendix.  3.4. Spatial Coherence Judgment  After iteratively optimizing object placement, a scene judge powered by GPT-4o evaluates the spatial alignment between the generated 3D scene and the image guidance produced during the layout refinement stage, ensuring consistency in inter-object relationships.  To assess this alignment, we design three metrics: (1) object category accuracy, comparing the generated scene  Figure 4. Collision avoidance and stability maintenance.  3.3.1. Pose Alignment  To address pose estimation errors from occlusions, segmen- tation, or asset mismatches, we adopt dense correspondence matching from RoMa [9], leveraging semantic spatial fea- tures for robustness to occlusions and partial views. Un- avoidable discrepancies in texture and shape between image guidance and retrieved assets are mitigated by focusing on high-level semantics over low-level details.  For each object, we match N correspondences between the rendered object and partially visible regions in the image guidance in 2D space. It then minimizes MSE loss on both 2D and 3D spatial locations of these N correspondences, backpropagating gradients to refine scale, translation, and upright rotation, as shown in Figure 3. Further details on pose estimation are provided in the Appendix.  3.3.2. Physical Plausibility  Real-world 3D scenes obey physical constraints, ensuring objects remain stable on contact surfaces and collision-free. However, pose alignment with image guidance alone does not guarantee physical plausibilityâ€”objects may intersect, float, or sink due to shape discrepancies and errors in scene understanding. See Figure 9 (b) as an example.  Existing methods approximate object geometry using 3D bounding boxes (3DBB) [47, 48], which oversimplifies shapes and leads to simplified inter-object relationships. For example, objects cannot be put within the shelf due to 3D bounding box collision. This results in simplified scene diversity, especially in tight spaces with complex inter-object relationships (see Figure 8 for an example). To address these challenges, we replace 3DBB-based approximations with Signed Distance Fields (SDFs), enabling precise object geometry representation for accurate collision detection and stability constraints.  The physical-aware optimization process iteratively constructs a SDF-based physical structure, following the scene graph hierarchy: processing the anchor object first to establish a stable foundation, followed by parent and child objects. The physics-aware optimization incorporates col- lision and stability constraints. Since retrieved 3D assets are upright, their rotation is constrained to azimuthal adjust- ments.  Formally, given a scene graph with N objects, each object has a 5-DoF configuration defined by scale s, upright rota- tion R, and translation T = (tx, ty, tz). For computational  5  ParentChildğ‘£ğ‘˜ğ‘‘(ğ‘£ğ‘˜)|ğ‘‘|ğ‘“(ğ‘»,ğ‘‘,ğ’–)ğ’–ğ‘»|ğ‘‘|ğ’–ğ‘”(ğ‘‘,ğ’–)ğ‘ ğ‘£ğ‘ğ‘£ğ‘ğ¿ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¿ğ‘ ğ‘ğ‘ğ‘™ğ‘’Optimized objectCollided objectParent objectğ¿ğ‘ ğ‘¡ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ğ‘£ğ‘ 4.1. Metrics  We evaluate controllability in text-based scene generation methods and three key properties essential for virtual con- tent generation: layout realism, physical plausibility, and interactivity. Controllability. Ensuring 3D scene generation aligns with input prompts is crucial. We assess this using: (1) CLIP Score [36] â€“ cosine similarity between image and text fea- tures from CLIP. (2) BLIP Score [22] â€“ image-text alignment using the ITM head of BLIPv2. (3) VQA Score [27] â€“ image- caption alignment based on VQA models. Layout Realism. Visual quality and spatial realism are im- portant to reflect real-world scene layouts. We evaluate it using following metrics: (1) Object Diversity â€“ number of objects and categories in the scene. (2) Layout Coherence â€“ adherence of object positions and orientations to common sense. (3) Spatial Realism â€“ presence of diverse spatial rela- tions (e.g., on top of, inside, under). (4) Overall Performance â€“ alignment of object categories and styles with the scene type. Evaluation details and examples are in the appendix. Physical Plausibility. Ensuring object collision-free and stable placement is fundamental for physical simulation en- vironments. We construct the following metrics: (1) Col-O â€“ average object collision rate, (2) Col-S â€“ average scene collision rate, (3) Inst-O â€“ average object instability rate, and (4) Inst-S â€“ average scene instability rate.  Collision is tested via mesh-mesh intersections, while in- stability follows Atlas3D [5], measured by tracking transfor- mations after physics-based simulation [31]. These metrics assess scene viability for virtual content creation. Interactivity. To ensure objects are accessible and manipu- lable in the scene based on their functional roles, we follow evaluation metrics from PhyScene [47]: (1) Reach â€“ average object reachability rate, and (2) Walk â€“ ratio of the largest connected walkable area over all walkable regions.  4.2. Quantitative Evaluation  Controllability. Table 1 presents a comprehensive evalua- tion of text-image alignment. Among all baselines, Scenethe- sis achieves the highest CLIP, BLIP, and VQA scores, con- firming its effectiveness in adhering to text description and the reliability of our agentic pipeline. Layout Realism. Table 1 reports visual realism scores from human evaluations and GPT-4o, a human-aligned eval- uator in text-to-3D tasks [16, 28]. Scenethesis achieves SOTA performance on most metrics. Despite DiffuScene and PhyScene being trained on dedicated indoor residential datasets [12], the training-free Scenethesis achieves compara- ble or superior layout realism in residential areas. In broader indoor settings (e.g., shopping centers, tourist attractions, sports facilities), Table 1 and Figure 5 show that Scenethe- sis significantly outperforms Holodeck in visual quality and spatial realism. These results demonstrate the advantages of  Figure 5. Human preference on diverse indoor scenes.  with the image guidance; (2) object orientation alignment, measuring how well object orientations match the reference layout; (3) overall spatial coherence, capturing the holistic consistency of the scene layout.  Each metric is normalized between 0 (lowest) and 1 (high- est). If any metric falls below a predefined threshold, the scene judge triggers a re-planning step. Further details are provided in the Appendix.  4. Experiment  Implementation. We use GPT-4o [16] as the LLM and im- age generation in vision module. Following Holodeck [48], we retrieve 3D models from a high-quality Objaverse [8] subset. Other module details are discussed in the above section. The physics-aware optimization is implemented by PyTorch [35] and PyTorch3D [38]. Experiments are run on an A100 GPU.  Baselines. Since we focus on interactive scene genera- tion, methods producing only single-geometry representa- tions are not relevant. We compare our approach against open-sourced state-of-the-art (SOTA) generative methods (DiffuScene [41], PhyScene [47]) and LLM-based methods (SceneTeller [33], Holodeck [48]). For fairness, all LLM- based methods use the same ChatGPT version.  Setup. Scenethesis generates both indoor and outdoor scenes (Figure 1, Figure 6), but for fair comparison, we evaluate only indoor scenes. To assess diversity and realism, we gen- erate 22 indoor scenes covering 6 primary and 12 secondary categories from DL3DV-10K [29]: Residential (living room, playroom, garage, warehouse), Shopping (bookstore, store), Tourism (museum, piano showroom), Sports (gym, billiard club), Medical (ward), Education (laboratory). Since Dif- fuScene, PhyScene, and SceneTeller were trained on indoor datasets [12] (mainly residential areas), we compare them within this domain. Holodeck, which also retrieves models from Objaverse, supports indoor scene generation, enabling comparisons across all indoor categories. To mitigate view- dependent bias, we render each scene from two perspectives, yielding 44 image pairs. For baselines lacking background generation (e.g. SceneTeller), we render Scenethesis outputs without an environment map for a fair comparison.  6  Shopping CentersTourist AttractionsSports and FitnessMedical FacilitiesResidential AreaEducation Institute020406080Human Preference (Percentage)HolodeckScenethesis Table 1. Quantitative evaluation on textâ€“image alignment and visual-quality preference (â†‘ higher is better). Bold marks the best for text control measurement. Visual quality preference indicates GPT-4o and human preference for our method over the baseline.  Method  Textâ€“Image Alignment CLIPâ†‘ BLIPâ†‘ VQAâ†‘  PhyScene DiffuScene SceneTeller Holodeck Ours  â€“ 23.11 25.27 28.32 30.71  â€“ 48.28 51.99 46.25 77.17  â€“ 0.7832 0.7999 0.6815 0.8269  Visual-Quality Preference of Ours (GPT-4o / Human Evaluation)  Object Diversityâ†‘  Layout Coherenceâ†‘  Spatial Realismâ†‘  Overall Performanceâ†‘  80% / 75% 75% / 80% 80% / 85% 85% / 80% â€“ / â€“  60% / 46% 80% / 90% 80% / 71% 83% / 78% â€“ / â€“  85% / 74% 90% / 76% 85% / 80% 81% / 86% â€“ / â€“  50% / 53% 80% / 80% 80% / 74% 85% / 85% â€“ / â€“  Figure 6. Qualitative results of generated indoor and outdoor scenes by Scenethesis. Scenethesis can generate diverse scenes given user prompts. Visualizations of the scenes at different camera viewpoints can be found in appendix.  Table 2. Physical-plausibility and interactivity results (â†“ lower is better for collision/instability). Bold indicates the best value.  Method  Physical Plausibility  Col-Oâ†“ Col-Sâ†“  Inst-Oâ†“  Inst-Sâ†“  Interactivity Reachâ†‘ Walkâ†‘  PhyScene DiffuScene SceneTeller Holodeck Ours  17.6% 19.5% 35.2% 6.1% 0.8%  51% 55% 75% 21% 6%  0.77 18.73% 75.22% 0.74 20.75% 83.33% 0.75 41.17% 78.57% 7.00% 31.58% 0.90 3.20% 16.67% 0.94  0.84 0.83 0.80 0.96 0.96  stability constraints, leading to frequent object intersec- tions. PhyScene [47] applies physical constraints but inherits dataset-induced collisions. Holodeck [48] prevents large- object collisions via Depth-First-Search solver but places small objects on predefined surfaces without collision checks, often causing inter-object penetrations (see appendix). More- over, none of these baselines address stability, resulting in frequent failures in physics-based simulations.  visual prior in guiding spatially realistic scene generation. Physical Plausibility and Interactivity. Table 2 presents object-level and scene-level physical plausibility metrics, demonstrating that Scenethesis significantly reduces colli- sions and enhances stability.  DiffuScene [41] and SceneTeller [33], trained on high- collision datasets [12, 47], lack collision detection and  In contrast, Scenethesis integrates physics-aware layout adjustment, ensuring low-collision, stable environments. Be- yond physical plausibility, Scenethesis excels in interactivity, achieving superior reachability and walkability scores. These results highlight Scenethesis â€™s ability to generate accessi- ble, navigable environments where objects align with their functional roles and afford interactions.  7  A private billiard room for relaxA living room with many reading materialsA beautiful surf-fishing beachA warehouse spaceThe museum exhibitionA reading corner Figure 7. Output Diversity. Given the same text prompt, Scenethesis can generate diverse scene with various objects and different layouts.  Table 3. Ablation study on the effectiveness of physically plausi- ble optimization. Scenethesis is the result in â€œ+Stabilityâ€ which includes all constraint components.  Component Raw layout +Pose Alignment +Collision +Stability  Pose Alignment â†‘ Collision Rate â†“ Instability Rate â†“ 22.7% 10.6% 3.6% 0.8%  87.3% 74.2% 69.8% 3.2%  0.536 0.732 0.755 0.836  Figure 8. Complex spatial realism. Spatial realism comparison between Scenethesis and Holodeck. Scenethesis generates spatially plausible 3D scenes, precisely placing small objects (e.g., bag, wine bottle, shoes, vase) within shelf compartments rather than just on top. This precision, challenging for LLM-based methods, is essential for embodied agent manipulation tasks [32, 47].  4.3. Qualitative Evaluation  Figure 6 showcases diverse scenes generated by Scenethesis, demonstrating high fidelity and versatility in both indoor and outdoor environments. Compared to LLM-based approaches, Scenethesis excels in realism and physical plausibility by leveraging image guidance and physics-aware optimization, effectively capturing real-world spatial complexity and di- versity. Figure 7 presents various 3D layouts generated from the same text prompt, highlighting diverse asset selection and spatial arrangements. Scenethesis supports both sim- ple and detailed promptsâ€”simple prompts enable flexible, user-friendly generation, while detailed prompts allow con- trollable 3D scene generation (see appendix).  Holodeck restricts small object placement to predefined areas on the top of larger objects. In contrast, Scenethesis enables fine-grained positioning, placing small object at dif- ferent levels of the support structure (e.g., shelves, carts), as  Figure 9. Effects of different constraints. (a) Scenethesis plans the layout and generates image guidance from text input. (b) Raw layout: places 3D models in estimated 3DBBs. (c) + Pose align- ment: adjusts 5DoF poses but lacks physical plausibility. (d) + Collision: prevents intersections but allows floating objects. (e) + Stability: ensures grounded, physically stable objects.  shown in Figure 8. LLM-based methods, which lack visual perception, struggle with this level of spatial realism. This capability is critical for embodied AI, enabling realistic in- teractions and meaningful object manipulation in simulation. More examples and qualitative comparisons can be found in appendix.  8  A modern car in the garageA home gym roomA children playroomScenethesisHolodeck(a). Image Guidance(b). Raw Layout(c). + Pose Alignment(d). + Collision(e). + Stability (full system) 4.4. Ablation Study  The physics-aware optimization has three components: pose alignment, collision constraint, and stability constraint. We perform ablation studies to assess their effectiveness. Metric. For each generated scene, we render the same view as the image guidance and use GPT-4o to assess pose align- ment based on: (1) object orientation, size, and position similarity, and (2) spatial coherence of the overall layout. The similarity score ranges from 0 to 1, with 1 indicating the highest alignment. Wall decorations are ignored in the comparison. Additionally, we evaluate object collisions and instability using the method in Section 4.2. Baselines. Raw Layout: Objects are placed based on 3DBB estimated by segmentation and depth prediction methods. Pose Alignment: Aligns object placement with image guid- ance via correspondence matching. Collision Constraint: Optimizes placement to avoid collisions. Stability Con- straint: Ensures objects remain stable. Results. As shown in Table 3, pose alignment significantly improves spatial consistency, while collision and stability constraints enhance physical plausibility, making scenes simulation-ready. Figure 9 shows qualitative visualization.  5. Conclusion and Limitation  We introduce Scenethesis, a training-free agentic framework for generating high-fidelity interactive 3D scenes by leverag- ing LLM-based coarse scene planning, vision-guided layout refinement, and physics-aware optimization for object po- sition adjustment. A scene judge module ensures spatial coherence. Experimental results demonstrate that it signif- icantly outperforms SOTA baselines in layout coherence, spatial realism, and plausibility. Our approach is limited by retrieval databases since generative 3D methods cannot yet handle articulation. Future advances in generative 3D could overcome this constraint by enabling articulated object synthesis, enhancing scene diversity.  9   Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation  Supplementary Material  6. Implementation Details of Scenethesis  6.1. Algorithm Overview  In this section, we provide a high-level algorithmic overview of Scenethesis, with detailed steps outlined in Algorithm 1.  6.2. Method Details  6.2.1. Coarse Scene Planning  Using the userâ€™s scene prompt as input, the LLM (powered by GPT-4o [16]) follows a six-step process: 1. Interpreting the userâ€™s scene prompt. 2. Reviewing the object categories available in the provided  asset database.  3. Selecting relevant objects from the asset list. 4. Cross-checking the availability of the selected objects. 5. Planning the scene using the selected objects. 6. Generating output files according to the specified stan-  dards. The final coarse scene planning output consists of two components: a list of selected object categories commonly found in the scene (defining anchor object and other com- mon objects) and an upsampled prompt that outlines the sceneâ€™s spatial hierarchy. The designed prompt presents in Coarse Scene Planning Instruction Prompts Section 7.1 and the output example is in Coarse Scene Planning Output Example Section 7.2.  6.2.2. Layout Visual Refinement  Based on the upsampled prompt, GPT-4o generates an im- age to serve as fine-grained layout guidance. Several post- processing steps are applied to the generated image: â€¢ Scene Graph Construction: GPT-4o [16] is used to gener- ate a scene graph, defining the ground as the root object, along with parent objects and their corresponding child objects. Additionally, Grounded-SAM [39] segments each object in the image to obtain masks and cropped images. These are then projected into 3D space using Depth Pro [2], allowing for the initial positioning of objects within a spa- tial relationship graph.  â€¢ Asset Retrieval. CLIP (ViT-L/14 trianed on LAION-2B) image and semantic features are employed to retrieve 3D assets that align with the image guidance. GPT-4o [16] is further utilized to select the most relevant environment map based on the upsampled prompt. It is important to note that Scenethesis focuses on layout planning for ob- jects on the ground, while background elements, such as wall decorations, lighting, or outdoor settings (e.g., sun- shine or the sea), are visually determined by the environ- ment map.  1  The output of the fine-grained layout planning includes the generated image as guidance, a scene graph with the initial poses of the objects, the retrieved assets, and the retrieved environment map. The visual details are presented in the video.  6.2.3. Physics-aware optimization Details  The physics-aware optimization is an iterative optimization process that consists of two key components: pose align- ment optimization and physical plausibility optimization. pose alignment optimization focuses on aligning the position, dimension, and orientation of 3D models with their coun- terparts in the image guidance to ensure visual coherence for spatial relationships. Physical plausibility optimization ensures that the 3D models in the scene are free from colli- sions and maintain stability, contributing to a realistic and physically consistent layout.  Pose Alignment. To align the position, dimension, and ori- entation for objects in rendered image and their counterpart in image guidance, Scenethesis applies the dense semantic correspondence matching from RoMa [9]. That is, mini- mizing the distance between correspondence points in the rendered image I and the guided image ËœI. Suppose there are N objects in the rendered image I, each represented by o and defined by a 5-DoF configuration, which includes scale s, upright rotation R, and translation T = (tx, ty, tz). The counterpart of each object in the generated image ËœI is denoted as Ëœo. The objective of ensuring visual coherence is to minimize the distance between corresponding points by optimizing the 5-DoF parameters. This ensures that the spatial positions, dimension, and orientations of the 3D mod- els are closely aligned with their counterparts in the guided image. The matching process is formalized as:  {p(x, y), Ëœp(x, y)}m  i = RoMa(o, Ëœo),  (4)  where p(x, y), Ëœp(x, y) are correspondent pair in object o and Ëœo. We select m pair points in each optimization iteration with confident score higher than Ï„ . The higher confidence score indicates a higher probability of matching. We minimize the 2D pixel distance and 3D projected point clouds distance between the matched pair denoted as follows:  Lpose = Î»2dL2d + Î»3dL3d,  (5)  where Î»2d and Î»3d are coefficients of the 2D pixel loss and 3D point cloud loss denoted as L2d and L3d.   Algorithm 1 Text to 3D Interactive Scene Generation  1: Input: User text 2: Output: 3D interactive scene layout 3:  Stage Stage 1: Coarse Scene Planning :  4: 5:  6:  7: 8: 9: 10: 11: 12:  13: 14:  15:  16: 17: 18: 19: 20: 21:  22:  23:  24: 25:  26:  27: 28: 29: 30:  31: 32:  33: 34: 35:  36: 37: 38:  39:  40: 41: 42: 43: 44:  object list, upsampled prompt â† LLM(user text)  â–· obtain the object list and an upsampled prompt  Stage Stage 2: Layout Visual Refinement :  img guidance â† 2D Diffusion (upsampled prompt) cropped images â† Grounded SAM (img guidance, object list) depth map â† Depth Pro (img guidance) 5DoF poses â† Extract Poses(cropped images, depth map) scene graph â† VLM (img guidance, object list, 5DoF poses) 3D assetsâ† CLIP (cropped images, object list) environment map â† VLM (upsampled prompt)  â–· generate the guidance image as the reference â–· identify each object and crop the images â–· generate depth map â–· generate initial 5DoF poses â–· generate scene graph â–· retrieve 3D assets â–· retrieve environment maps  Stage Stage 3: Physics-aware Optimization:  scene SDF â† Init Scene SDF(anchor object) for node in scene graph.bfs traverse() do  s, R, T â† node.pose parent SDF â† node.parent.SDF for iteration = 1 to max iterations do  mesh â† Get Object Mesh(node) meshâˆ— â† Apply Transform(mesh, s, R, T)  â–· compute SDF for each object â–· iterate over all objects â–· variables to be optimized â–· obtain parent objectâ€™s SDF  â–· coordinate alignment  img rendered, depth rendered â† Render(meshâˆ—, camera)  â–· render RGB and depth images  correspondence â† RoMa(img guidance, img rendered) mesh points â† Get Point Clouds(depth rendered, correspondence, camera) guided points â† Get Point Clouds(depth map, correspondence, camera)  â–· correspondence matching  Lpose 2D â† Dist 2D(correspondence) Lpose 3D â† Dist 3D(mesh points, guided points) Lcollision â† Collision(meshâˆ—, scene SDF) Lstability â† Stability(bottom points(mesh), parent SDF) loss â† Î»Lpose + Î»collisionLcollision + Î»stabilityLstability loss.Backward()  optimizer.Step() optimizer.Zero Grad()  end for scene SDF â† Update Scene SDF(scene SDF, node)  end for  Stage Stage 4: Scene Spatial Coherent Judgment:  Multi-view images â† Render (optimized 3D scene) Qualified â† VLM (Multi-view images) if not qualified then goto Stage 1  end if  â–· loss computation  â–· pose optimization  â–· re-generate if current optimization fails.  45: 46: Return: Optimized 3D interactive scene  2   (a). Ltranslation  (b). Lscale  (c). Lstability  Figure 10. Illustration of collision avoidance and stability maintenance. The solid-line circle indicates the 3D objectâ€™s current position, while the dotted-line circle marks its anticipated position. The black dot represents the centroid of the target object, the purple dots indicate surface nodes with negative SDF values, and the red point vk is the bottom node of the object. (a). The collision pushes the circle object out of the rectangle along the direction from the sampled point to the circleâ€™s center by step |d|. (b). The collision indicates the object is too large and negative signed distance fields (SDF) points (i.e. point va and point vb) are detected from distinct classes of the object during optimization. The collision loss shrinks the object size such that there are no different clusters of negative SDF points on the object surface can be detected. (c). The stability maintenance keeps the the child and the parent to be as close as possible.  Physical Plausibility. Physical plausibility ensure gener- ated 3D scenes adhering to fundamental physical principles. Instead of using 3D bounding box (3DBB) as object approx- imation, Scenethesis accurately detects collision state from the surface points of the 3D models using signed distance field (SDF). The collision avoidance and stability mainte- nance as illustrated in Figure 10.  The collision avoidance affects the translation T by:  Ltranslation = ||f (T, |d|, u) âˆ’ T||2 2,  (6)  where f (T, d, u) = T + u Â· |d| computes a collision-free position Ë†T by adjusting T along direction u with step size d. Here, d is the negative SDF value at a collided point vi such that d(vi) â‰¤ 0 and |d| = max(0, âˆ’d(vi)) is the negative SDF value d after being processed through a ReLU function, meaning only collided points contribute to this collision term. The direction u is defined from the collision point toward the modelâ€™s centroid C, guiding objects away from collisions. The collision avoidance affects the scaling s by detecting  that object collides from at least two different directions:  Lscale =  ï£± ï£´ï£²  (cid:80)  viâˆˆVâˆ’  ï£´ï£³  0  (cid:18)  (cid:19)2  g(|di|, ui) âˆ’ s  if Ncluster > 1  ,  The stability maintenance affects the translation T by:  Lstability =  (cid:18)  (cid:88)  viâˆˆVB  (cid:19)  1 âˆ’ exp(âˆ’d2 i )  ,  (8)  where VB are the sampled points on the bottom surface of bounding box, and di are their corresponding SDF values.  Method Overview Building on the physics-aware opti- mization described above, we now integrate pose spatial constraints and physical constraints into the text-to-3D opti- mization framework. Since physical loss depends on object geometry and can alter its spatial position, it may affect the rendered visible regions due to occlusions or shifts in the scene, introducing biases in pose alignment when using im- age guidance for semantic correspondence matching. To mitigate this, we adopt a two-stage optimization strategy: first, we optimize pose alignment based on correspondence matching; then, we refine object placement with physical constraints to ensure a visually coherent and physically plau- sible 3D scene. The following function defines the joint optimization of object position, orientation, and scale:  otherwise  L = Î»pLpose +Î»c T Ltranslation +Î»c SLscale +Î»sLstability (9)  ||ui||  (7) where g(|di|, ui) = ||ui||âˆ’|di| defines the target scale to re- duce collision regions. Ncluster denotes the number of distinct clusters formed without SDF sign flipping. As shown in Fig- ure 4, two surface points i and j with di â‰¤ 0 and dj â‰¤ 0 belong to different clusters, and thus push the object to be smaller.  6.3. Experiment Details  Parameters. For pose alignment, we select the m = 100 correspondence points with matching conference Ï„ â‰¥ 0.6 in each optimization iteration. Additionally, we uniformly select n = 400 samples from the surface of 3D model to  3  |ğ‘‘|ğ‘“(ğ‘»,ğ‘‘,ğ’–)ğ’–ğ‘»|ğ‘‘|ğ‘£ğ‘|ğ‘‘|ğ’–ğ‘”(ğ‘‘,ğ’–)ğ‘ ğ‘£ğ‘ğ‘£ğ‘ParentChildğ‘£ğ‘˜ğ‘‘(ğ‘£ğ‘˜) Figure 11. Short prompt: a living room with reading materials; detailed long prompt: A living room that provide a neutral and cozy space with a minimalist design. At the center of the scene, a light beige sofa is positioned against a textured stone wall in the background. In front of the sofa, a round wooden coffee table sits on the floor, with a white coffee cup placed on top. Two blue armchairs are symmetrically arranged on either side of the coffee table, facing inward toward the sofa. Behind each armchair, a tall white floor lamp stands, providing ambient lighting. Next to the lamps, green potted plants are placed near the wall, adding a natural decorative touch.  accurately detect the collision and stability states in each optimization iteration.  We explored Adam and SGD as the optimizer during the optimization process. Though Adam has been widely applied for training deep neural networks, the adaptive momentum makes the optimization unstable, leading to sub-optimal optimized pose. So we use SGD in our implementation. The optimization implementation is based on pytorch3D [38] and the visualization is rendered using Blender.  Prompts. Scenethesis supports both short and detailed user-specified prompts. A short prompt provides a user- friendly and flexible approach to 3D scene generation, where the LLM interprets the input, revisits the available 3D models in database, selects the common objects and anchor objects, and generates an upsampled text prompt for coarse layout planning. In contrast, a long prompt, which includes user- defined objects and inter-object relationships, enables greater user control over 3D scene generation. In this case, the LLM directly reasons over the detailed prompt, revisits available 3D models in the database, and defines the anchor object, skipping the upsampling stage. We illustrate examples of short and long prompts defining a living room in Figure 11. [47], We and Diffuscene Holodeck [48]â€”evaluating visual quality, physical plausibility, and interactivity metrics. Among them, Diffuscene, SceneTeller, and Holodeck perform text-to-3D scene generation. For visual quality assessment, we use both a user study and GPT-4o as evaluation tools. Unlike other baselines, which generate only living room, bedroom,  baselinesâ€”Physcene  SceneTeller  compared  [41],  [33],  four  Figure 12. User study example.  and dining room scenes from 3D-FRONT [12], Holodeck and Scenethesis utilize Objaverse [8] as a retrieval database, enabling more diverse indoor scene generation.  We outline the GPT-4o prompt assessment for both base-  line evaluation and ablation evaluation as follows:  â€¢ Comparison with baselines by GPT-4o: GPT-4o is em- ployed to evaluate the generated scenes for four metrics: object diversity, layout coherence, spatial realism and complexity, and overall performance. The evaluation prompts are detailed in the Instruction Prompts for Evalu- ating Generated Scene Section 7.3. Additionally, a com- parison example of the generated scenes is provided in Figure 13 with their evaluation results generated by GPT- 4o detailed in Evaluation Example of Generated Scenes Section 7.4.  â€¢ Comparison with baselines by human preference: We applied a user study to study human preference of baseline method with our method. See Figure 12 as an example. There are 69 users took our survey.  â€¢ Evaluation in Ablation Studies: GPT-4o is also utilized to assess the pose alignment metric during the ablation studies of Scenethesisâ€™s physics-aware optimization. This evaluation measures the similarity of object position, size, and orientation with their counterparts in the image guid- ance, as well as the overall visual coherence of the layout. The instructions for assessing pose alignment are provided in the Instruction Prompts for Ablation Study Section 7.5.  4  User specified long prompt caseUser specified short prompt case periment section, scenes generated by Scenethesis demon- strate greater diversity in object categories, quantities, and sizes. More importantly, Scenethesisâ€™s scenes have a broader range of spatial relationships, such as â€œon top ofâ€, â€œinsideâ€, and â€œunderâ€, compared to those generated by Holodeck [48], which supports only â€œon top ofâ€ spatial relation. Furthermore, Scenethesisâ€™s scenes align more faithfully with the intended scene type. i.e. when given the description â€œa peaceful beach during sunsetâ€, Scenethesis produces an outdoor scene with appropriate beach ele- ments, while Holodeck incorporates beach-related objects but generates an environment resembling an indoor setting.  Figure 13. An example comparison of generated scenes given user prompt: â€œa warehouseâ€. Note that Scenethesisâ€™s scenes are rendered without an environment map to ensure a fair comparison with Holodeckâ€™s scenes.  Figure 14. An example of objects collision from Holodeckâ€™s scenes.  Results. We present additional qualitative results for Scenethesisâ€™s scenes. â€¢ Qualitative Results of Scenethesisâ€™s Scene: We present different camera views to showcase the qualitative results of Scenethesisâ€™s scenes, as shown in Figure 15. Figure 17 presents the generated scenes by Scenethesis with their image guidance. Note that Scenethesis focuses on layout planning for ground objects. The absence of certain unique assets in Objaverse [8] may cause discrepancies between the generated scene and the image guidance. Future work could address this by incorporating more diverse assets. â€¢ Quantitative Results of Physical plausibility Compar- ison: The physical Plausibility quantitative comparison presented in Table 1 of the Experiment section. While Holodeck applies both soft and hard constrains based on the Depth-First-Search Solver and small objects are placed on predefined locations. These small objects may collide with each other due to the shape and size variations as shown in Figure 14.  â€¢ Visual Comparison with Holodeck: In addition to the quantitative comparison presented in Table 1 of the Exper- iment section, we provide a visual comparison between Scenethesis and Holodeck, a state-of-the-art LLM-based 3D interactive scene generation method, in Figure 16. Based on the four evaluation metrics detailed in the Ex-  5  ScenethesisHolodeck Figure 15. Qualitative results of generated indoor and outdoor scenes by Scenethesis at different camera viewpoints  6  A living room with reading materialsA private billiard room for relaxA beautiful surf-fishing beachThe museum exhibitionA warehouse spaceA reading corner Figure 16. Visualization comparison of generated scenes between Scenethesis and Holodeck. The first column of images shows scenes generated by Scenethesis without an environment map, the second column displays scenes generated by Scenethesis with an environment map, and the third column presents scenes generated by Holodeck. The evaluation metrics, including object diversity, layout coherence, spatial realism, and overall performance, are detailed in the Experiment section. Scenethesisâ€™s scenes have a wider variety of spatial relationships, such as â€œon top ofâ€, â€œinsideâ€, and â€œunderâ€, compared to those generated by Holodeck [48], which supports only â€œon top ofâ€ spatial relation. In addition, Holodeck lacks visual perception and usually generates misoriented objects, e.g. shelves occlude the window in children playroom and warehouse, chair orients towards the window in the hospital case, hindering their functionalities.  7  A children playroomA museum exhibitionA living room with reading materialsA warm hospital wardA peaceful beach during sunsetA cute piano roomA spacious warehouseA museum exhibitionScenethesis scene w.o/w. environment mapHolodeck sceneScenethesis scene w.o/w. environment mapHolodeck scene Figure 17. We provide a visual illustration of the generated scenes and their corresponding image guidance. The first column displays the image guidance, while the second and third columns show the generated scenes without and with the environment map, respectively. Note that Scenethesis focus on layout planning for objects on the ground. Additionally, certain unique assets, such as a beach mat, are unavailable in Objaverse [8], which may result in the generated scene differing from the image guidance. Future work could enhance the system by incorporating a wider range of assets.  8  A living room with reading materialsA peaceful beach during sunsetA cute piano roomA warm hospital wardA realistic laboratory sceneA relax beach sceneImage guidanceScenethesis scene w.o./w. environment mapImage guidanceScenethesis scene w.o./w. environment map 7. Prompts Examples  7.1. Coarse Scene Planning Instruction Prompts  Coarse Scene Planning Instruction Prompts  Task Description: You are responsible for generating a set of common objects and planning a scene based on these common objects. You will be given a list that includes all available object categories and a text prompt to describe a scene. This is a hard task, please think deeply and write down your analysis in following steps: Step 1: Review All Categories  a. Begin by thoroughly reviewing the categories in the provided list. b. Identify potential groups or clusters of objects within this list that are commonly found in similar  environments (e.g., furniture, electronics, household items, etc.).  Step 2: Interpret Input Prompt  a. Carefully read the input prompt. Understand the theme, primary activities, or the setting it describes, as these will guide your object selection. i.e. if the prompt gives: children playing room, then you may think of objects like tent, toy, bear, ball, chair, etc.  Step 3: Object Selection  a. Based on the description, select at least 15 object categories from the list that match the scene. b. Determine the anchor object:  i. Identify the anchor object among the selected objects. Consider the following factors:  1. A large object directly on the ground (i.e. floor, table, or shelf). 2. An object that influences where other objects are placed (i.e. a table in a dining room, and there  are cups and fruits on the table).  3. The object should logically anchor the scene and often defines the sceneâ€™s layout orientation. i.e.  Step 4: Object Cross-check  the sofa in a front-facing view in the scene.  a. I will give you $100 tips if you can cross-check whether objects in the scene can be found in the given category list or its relevant categories. i.e., if there is a bookshelf in your planned scene, the bookshelf should also be found in the given list, or bookcase can be found in the list if bookshelf is not covered by the category. Otherwise, re-plan the scene.  Step 5: Plan Scene with Selected Objects  a. Based on the description and selected objects, plan the scene, keeping these aspects in mind:  i. Functionality: Choose objects that are contextually relevant to the scene (e.g., selecting a table,  chair, flower vase, and utensils for a dining room), but do not generate any wall dÂ´ecor objects.  ii. Spatial Hierarchy:  1. Please have a depth effect in the layout. For the depth effect, the scene should have some objects placed on the ground as the background, central, and in the front, resulting in a depth layout. i.e. the sofa and bookshelf are the background of the table and chair set in the living room.  2. Please have a supportive item in the layout. i.e. the shoes, bag, and hat are in the display shelf in  a clothes store, where the display shelf is a supportive item.  iii. Balance: Ensure a mix of large and small objects to avoid overcrowding or under-populating the scene. i.e. taking the table as the center, there are flower vases, fruits, and cups on the table, and chairs are on the sides.  Step 6: Output Format:  a. Save the selected objects as a json file follow the output format:  Anchor object: Other common objects: b. Save scene planning as txt file.  9   7.2. Coarse Scene Planning Output Example  Coarse Scene Planning Output Example  User input: a gym room  Step 1: Reviewing Object Categories for Gym-Related Items  From the provided list, here are the relevant categories typically found in a gym setting: â€¢ Exercise equipment: dumbbell, treadmill, weight, barbell â€¢ Furniture or storage items: bench, shelf, cabinet â€¢ Accessories and smaller items: towel, water bottle, yoga mat, scale, bag  Step 2: Scene Interpretation  The scene is a gym room, so the primary objects would focus on equipment for strength and cardio exercises, along with supportive items like shelves for organization and accessories like towels and water bottles for hydration.  Step 3: Scene Layout and Object Selection  â€¢ Anchor object: Treadmill (as the main focus of cardio equipment) â€¢ Other common objects: Dumbbell, Barbell,Bench, Shelf, Water bottle, Towel, Weight, Bag, Mat, Scale,  Cabinet, Exercise ball, Fan, Mirror  Step 4: Object cross-check  â€¢ Treadmill - âœ“ Available as â€treadmillâ€ â€¢ Dumbbell - âœ“ Available as â€dumbbellâ€ â€¢ Barbell â€“ âœ“ Available as â€barbellâ€ â€¢ Bench - âœ“ Available as â€benchâ€ â€¢ Shelf - âœ“ Available as â€shelfâ€ â€¢ Water bottle â€“ âœ“ Available as â€water bottleâ€ â€¢ Towel â€“ âœ“ Available as â€towel rackâ€ â€¢ Weight â€“ âœ“ Available as â€weightâ€ â€¢ Bag â€“ âœ“ Available as â€bagâ€ â€¢ Mat â€“ Not explicitly listed, but a close alternative like â€place matâ€ â€¢ Scale â€“ âœ“ Available as â€scale (measuring instrument)â€ â€¢ Cabinet â€“ âœ“ Available as â€cabinetâ€ â€¢ Exercise ball â€“ Not explicitly listed, but a close alternative like by â€ballâ€ â€¢ Fan â€“ âœ“ Available as â€fanâ€ â€¢ Mirror â€“ âœ“ Available as â€mirrorâ€ Step 5: Plan Scene with Selected Objects  â€¢ Background: Shelf and cabinet are placed to the side or back corner of the room for organization, holding smaller items. Mirror mounted on the back wall for checking exercise form. They serves as supportive equipments...It enhances depth and makes the room feel more spacious...  â€¢ Middle Ground: Treadmill is the primary cardio equipment and anchor object, anchoring the scene. Other objects such as bench and barbell are placed around the treadmill, allowing access from multiple directions for various exercises. They provide visual weight to the scene and represent strength-focused equipment... â€¢ Foreground: Dumbbell, water bottle and other objects set closer to the viewer, easily accessible for hand- held exercises. It introduces a smaller, more detailed object in the front. They placed nearby, adding another layer of exercise options for core stability...  This hierarchy ensures a balanced layout, where larger and bulkier items form the background... The middle ground includes the treadmill and training equipment, while smaller and accessible items like dumbbells occupy the foreground... This setup provides a clear layout for practical use and visual depth in the gym scene...  Step 6: Output Format A json file that include the available objects and a txt file that describe the scene planning.  10   7.3. Instruction for Evaluating Generated Scenes  GPT-4o Evaluation Prompts for Generated Scenes  Evaluate Generated Scenes for Scenethesis and Holodeck This is a hard problem. You are supposed to compare the alignment of a pair of images with a given text prompt that describe the scene. Images contain generated scenes by two different methods. Please evaluate them in the following five aspects: 1. Object diversity: Counting number of object and object types in the scene. The higher number, the better object diversity. i.e. if there are three shelves and a box in the scene. Then the object type is 2 and the number of counting is 3+1 = 4.  2. Layout coherence: whether the objects position and orientation in the scene are realistic and adhere to common  sense. â€¢ An ideal layout would be dependents on the scene type. i.e. the objects in the garage scene will forms as a bit chaotic organization and it would be less reasonable if all objects standing against the wall in a very clean order.  â€¢ Objects should be placed reasonably. i.e. shelves fly in the air or hange on the wall.  3. Spatial realism & complexity: whether scene contains diverse hierarchy. The measure of spatial complexity is by review scenes and carefully evaluate objects relations. The higher diverse relation indicate a better spatial complexity. The spatial hierarchy refers to the following aspects: â€¢ relations such as on the top of, in side of, under the, etc. â€¢ objects in various sizes. i.e. the scene should contain large objects such as shelf and small objects such as apple. â€¢ objects that are arranged in different area and form a depth effects. i.e. objects are in the background, in the front,  and in the center of the space.  4. Overall performance: assetsâ€™ categorizes and styles are faithful to the scene type. i.e. fruits are less likely in the warehouse, while carts, boxes are more likely in a warehouse scene. Thus, it would be a bad performance if find fruits in the warehouse and good performance if cartsshelfboxes presents in the warehouse. Please carefully evaluate the objectâ€™s category and scene type. To see whether it is reasonable enough based on your common sense. 5. Please ignore aspects in : environment background style such as lighting, shadow, color, and decorations in the wall  etc. This is a challenging task, please think aloud and make your decisions by: â€¢ analyzing each criteria one by one for each image. â€¢ report your evaluation results based on the analysis.  6. Your output format should be: {} is better than {} in the criteria {}.  11   7.4. Evaluation Example of Generated Scenes  GPT-4o Evaluation Results  Scenethesisâ€™s Scene 1. Object diversity:  â€¢ Objects visible: Ladder, barrel, boxes, fire extinguisher, table, tire, crate. â€¢ Object types: 8 types. Total count: 13 â€¢ Higher diversity due to various objects.  2. Layout coherence:  â€¢ Objects are dispersed naturally, suggesting activity. â€¢ Different sizes and a less structured arrangement suggest a more realistic warehouse scene.  3. Spatial realism & complexity:  â€¢ Objects have various spatial relations; ladder stands, boxes on ground, tables at center... â€¢ Good diversity with a mix of background and central objects.  4. Overall performance:  â€¢ Varied objects like ladders, barrels, crates, and extinguishers are typical in a warehouse. â€¢ Realistic depiction of a warehouse setting.  Holodeckâ€™s Scene 1. Object diversity:  â€¢ Objects visible: Shelves, cabinets, safety vests, and workbenches. â€¢ Object types: Shelves, cabinets, vests, workbenches. Total count: 7+2=9 (shelves are considered per structure, as  are others).  â€¢ Diversity is somewhat low as there are few types.  2. Layout coherence:  â€¢ Organization is neat; workbenches and cabinets are along the walls. â€¢ Reasonable layout for a tidy scene, but lacks variety in sizes.  3. Spatial realism & complexity:  â€¢ Shelving and cabinets against walls create vertical space; workbenches on the ground. â€¢ Lacks diverse spatial relations.  4. Overall performance: Objects like shelves and workbenches fit a warehouse setting, but it lacks variety typical in  larger-scale warehousing.  Evaluation Object diversity: Scenethesis is better than Holodeck; Layout coherent: Scenethesis is better than Holodeck; Spatial realism & complexity: Scenethesis is better than Holodeck; Overall performance: Scenethesis is better than Holodeck.  12   7.5. Instruction Prompts for Ablation Study  Pose Alignment Evaluation Instruction Prompt  This task involves evaluating the pose alignment between two images in a pair. One image serves as the image guidance (GT), while the other is a generated image. Your objective is to measure the pose alignment of the generated image relative to the GT image. Follow these steps for evaluation: 1. Review Objects in the GT Image: Examine all objects in the GT image, focusing on their locations, sizes, and  orientations. Understand the spatial relationships among objects, such as on top of, inside, under, etc.  2. Evaluate pose alignment: Assess the similarity between the generated image and the GT image based on the  following three aspects: â€¢ Location and Size Similarity: Compare the location and size of objects in the generated image with those in the GT image. Assign a similarity score between 0 and 1, where 1 indicates the highest similarity. For example: â€“ If an apple in the GT image is placed at the center of a table, and in the generated image it is placed on the left  side of the table, the similarity might be moderate (e.g., 0.5).  â€“ If the apple is misplaced (e.g., on the ground or missing entirely), the similarity would be very low (e.g., 0.1). â€¢ Orientation Similarity: Examine the orientation of each object in the generated image compared to the GT image. Pay close attention to details, noting any deviations such as slight tilts (e.g., right/left, up/down) or rotations that create different perspectives. Assign a score from 0 to 1, where 1 indicates perfect alignment and 0 indicates a significant mismatch (e.g., opposite orientation).  â€¢ Overall Layout Similarity: Assess the overall visual coherence of the generated image compared to the GT image, including spatial relationships and hierarchical structure. Assign a similarity score between 0 and 1, where 1 represents a perfect match. For instance: â€“ A perfect match occurs when the generated image maintains the same spatial relationships, relative locations,  sizes, and orientations as the GT image (e.g., an apple placed at the center of a table in both images).  â€“ Small deviations in placement or orientation are acceptable but should result in a lower score.  3. Exclusions: Do not consider style, appearance, object shape, or texture in your evaluation. Focus solely on pose  alignment.  4. Output Format: Clearly document your similarity scores for each aspect (Location and Size Similarity, Orientation Similarity, and Overall Layout Similarity) following the format: location and size similarity score is {}, orientation similarity score is {}, and overall layout similarity score is {}. Please save the evaluated scores as a json file.  13",
    "url": "https://arxiv.org/pdf/2505.02836v1.pdf"
  }
]