import gradio as gr
import os
import re
import json
import time
import threading
from dotenv import load_dotenv
from agent import agent
from datetime import datetime

from function_dev.pdf_creator import export_json_to_pdf
from function_dev.email_sender import send_email_with_pdf
from function_dev.json_to_vectordb import run_vector_pipeline
from module.wrapper import (
    News_pipeline_wrapped,
    Blogs_pipeline_wrapped,
    Paper_pipeline_wrapped,
    set_web_params
)
from langchain_community.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from langchain.tools import StructuredTool


# ì†ŒìŠ¤ ì¶”ë¡ 
def infer_source_type(prompt: str) -> str:
    prompt = prompt.lower()
    if any(k in prompt for k in ["ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "news"]):
        return "news"
    elif any(k in prompt for k in ["ë¸”ë¡œê·¸", "blog"]):
        return "blog"
    elif any(k in prompt for k in ["ë…¼ë¬¸", "paper", "í•™ìˆ ìë£Œ"]):
        return "paper"
    return "unknown"

# ìš”ì•½ ê²°ê³¼ íŒŒì‹±
def parse_news_output(output: str):
    blocks = output.strip().split("\n\n")
    parsed_items = []

    for block in blocks:
        title_match = re.search(r"\*\*(.+?)\*\*", block) or re.search(r"\[(.+?)\]\(", block)
        url_match = re.search(r"\((https?://[^\s]+)\)", block)
        summary_match = re.findall(r"-\s(.+)", block)

        if title_match and url_match:
            parsed_items.append({
                "title": title_match.group(1).strip(),
                "url": url_match.group(1).strip(),
                "summary": " ".join(summary_match).strip() if summary_match else ""
            })

    return parsed_items

# í™˜ê²½ ì„¤ì •
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

tools = [
    StructuredTool.from_function(News_pipeline_wrapped, name="crawl_news",
        description="ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ìš”ì•½í•˜ëŠ” íŒŒì´í”„ë¼ì¸. 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„', 'news' ìš”ì²­ ì‹œ ì‚¬ìš©"),
    StructuredTool.from_function(Blogs_pipeline_wrapped, name="crawl_blog",
        description="ë¸”ë¡œê·¸ ê¸€ì„ í¬ë¡¤ë§í•˜ê³  ìš”ì•½í•˜ëŠ” íŒŒì´í”„ë¼ì¸. 'ë¸”ë¡œê·¸', 'blog' ìš”ì²­ ì‹œ ì‚¬ìš©"),
    StructuredTool.from_function(Paper_pipeline_wrapped, name="crawl_papers",
        description="ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ê³  ìš”ì•½í•˜ëŠ” íŒŒì´í”„ë¼ì¸. 'ë…¼ë¬¸', 'paper', 'í•™ìˆ ìë£Œ' ìš”ì²­ ì‹œ ì‚¬ìš©")
]

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, openai_api_key=OPENAI_API_KEY)

system_message = """
ë„ˆëŠ” ë‰´ìŠ¤, ë¸”ë¡œê·¸, ë…¼ë¬¸ ìë£Œë¥¼ í¬ë¡¤ë§í•˜ê³  ìš”ì•½í•˜ëŠ” AI ì—ì´ì „íŠ¸ì•¼.
ì‚¬ìš©ìì˜ ìš”ì²­ì´ ë‰´ìŠ¤, ë¸”ë¡œê·¸, ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì ì ˆí•œ íˆ´ì„ ì‹¤í–‰í•´ì•¼ í•´.

ë§Œì•½ ìš”ì²­ì´ ì´ ì„¸ ê°€ì§€ì™€ ê´€ë ¨ì´ ì—†ë‹¤ë©´ ì´ë ‡ê²Œ ë‹µí•´ì•¼ í•´:
"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‰´ìŠ¤, ë¸”ë¡œê·¸, ë…¼ë¬¸ì— ëŒ€í•œ ìš”ì²­ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
"""

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True,
    agent_kwargs={"system_message": system_message.strip()}
)


# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_pipeline(prompt, country, synonym_range, email):
    start_time = time.time()
    result_holder = {"done": False, "result": None}

    def task():
        set_web_params(synonym_range, country)
        result = agent.invoke(prompt, return_only_outputs=True)
        print(result)
        parsed = parse_news_output(result["output"])
        print(parsed)
        result_holder["result"] = parsed
        result_holder["done"] = True

        # JSON ì €ì¥
        base_dir = os.path.dirname(os.path.abspath(__file__))
        save_dir = os.path.join(base_dir, "database")
        os.makedirs(save_dir, exist_ok=True)
        
        source_type = infer_source_type(prompt)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = os.path.join(save_dir, f"{source_type}_summary_{timestamp}.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(parsed, f, ensure_ascii=False, indent=4)

        # PDF + ì´ë©”ì¼ ì „ì†¡
        if email.strip():
            pdf_path = export_json_to_pdf(parsed)
            send_email_with_pdf(email, pdf_path)

        # ë²¡í„°í™”
        run_vector_pipeline(database_dir=save_dir)

    thread = threading.Thread(target=task)
    thread.start()

    # ì‹¤ì‹œê°„ ìƒíƒœ í‘œì‹œ
    while not result_holder["done"]:
        elapsed = int(time.time() - start_time)
        yield f"<div style='text-align:center; margin-top: 20px;'>â³ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤...<br><small>ê²½ê³¼ ì‹œê°„: {elapsed}ì´ˆ</small></div>"
        time.sleep(1)

    # ìµœì¢… ê²°ê³¼ í‘œì‹œ
    elapsed = int(time.time() - start_time)
    output_md = f"<div style='text-align:center; margin-top: 20px;'>âœ… <b>ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!</b><br><small>ì´ ê²½ê³¼ ì‹œê°„: {elapsed}ì´ˆ</small></div><br><br>"
    for i, item in enumerate(result_holder["result"], 1):
        output_md += f"**{i}. [{item['title']}]({item['url']})**\n\n- {item['summary']}\n\n"
    yield output_md


# Gradio UI ì •ì˜
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§  Daily AI Briefing Assistant")

    with gr.Row():
        prompt = gr.Textbox(label="ğŸ“ í”„ë¡¬í”„íŠ¸", placeholder="ì˜ˆ: ìµœê·¼ 2ì¼ê°„ AI ê´€ë ¨ ë‰´ìŠ¤ ì •ë¦¬í•´ì¤˜")
        country = gr.Dropdown(["Korea", "Japan", "China", "USA", "Europe"], value="Korea", label="ğŸŒ êµ­ê°€ ì„ íƒ")
        synonym_range = gr.Slider(1, 5, step=1, value=3, label="ğŸ” ê²€ìƒ‰ ë²”ìœ„")
        email = gr.Textbox(label="ğŸ“§ ì´ë©”ì¼ (ì„ íƒ)", placeholder="ê²°ê³¼ë¥¼ ì´ë©”ì¼ë¡œ ë°›ê³  ì‹¶ë‹¤ë©´ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    submit = gr.Button("ğŸš€ ì‹¤í–‰")
    output = gr.Markdown(label="ğŸ“„ ê²°ê³¼")

    submit.click(fn=run_pipeline, inputs=[prompt, country, synonym_range, email], outputs=output)

if __name__ == "__main__":
    demo.launch(share=True)
