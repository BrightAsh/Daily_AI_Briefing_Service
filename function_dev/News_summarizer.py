import torch
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
from sentence_transformers import SentenceTransformer, util

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… KoBART ëª¨ë¸ ë¡œë“œ
kobart_model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization').to(device)
kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')

# âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# âœ… ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
def split_text_into_sentences(text):
    import re
    return re.split(r'(?<=[.!?])\s+', text)

# âœ… ë¬¸ì¥ í† í° ê¸°ì¤€ ê·¸ë£¹í™”
def group_sentences_by_token_limit(sentences, tokenizer, max_tokens):
    groups = []
    current_group = ""
    for sentence in sentences:
        tentative_group = current_group + " " + sentence if current_group else sentence
        tokenized = tokenizer.encode(tentative_group, return_tensors="pt")
        if tokenized.size(1) <= max_tokens:
            current_group = tentative_group
        else:
            if current_group:
                groups.append(current_group.strip())
            current_group = sentence
    if current_group:
        groups.append(current_group.strip())
    return groups

# âœ… ìš”ì•½ í•¨ìˆ˜
def summarize_kobart(text, max_input_length=1024, max_output_length=700):
    inputs = kobart_tokenizer.encode(text, return_tensors="pt", max_length=max_input_length, truncation=True).to(device)
    summary_ids = kobart_model.generate(
        inputs,
        max_length=max_output_length,
        min_length=100,
        num_beams=4,
        no_repeat_ngram_size=3,
        repetition_penalty=2.0,
        length_penalty=1.0,
        early_stopping=False
    )
    return kobart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# âœ… ìœ ì‚¬ë„ í•„í„° í•¨ìˆ˜
def is_relevant(summary: str, keyword: str, threshold=0.1) -> bool:
    embeddings = embedder.encode([summary, keyword])
    similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
    print(f"    ğŸ” ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.4f} (í‚¤ì›Œë“œ: {keyword})")
    return similarity >= threshold

# âœ… ê³„ì¸µì  ìš”ì•½ í•¨ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í¬í•¨, ì¤‘ë³µ ì œê±° X)
def hierarchical_summary(full_text, keyword=None, max_input_length=1024):
    sentences = split_text_into_sentences(full_text)
    text_chunks = group_sentences_by_token_limit(sentences, kobart_tokenizer, max_input_length)
    chunk_summaries = []

    for i, chunk in enumerate(text_chunks, 1):
        print(f"ğŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} ìš”ì•½ ì¤‘...")
        summary = summarize_kobart(chunk)
        chunk_summaries.append(summary)
        print(f"ğŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} summary: {summary}")

    if len(chunk_summaries) == 1:
        # âœ… ì²­í¬ê°€ í•˜ë‚˜ë¼ë©´ â†’ ìš”ì•½ í•œ ë²ˆë§Œ ë°˜í™˜
        final_summary = chunk_summaries[0]
        print("âœ… ì²­í¬ 1ê°œ â†’ ì¶”ê°€ ìš”ì•½ ì—†ì´ ë°˜í™˜")
    else:
        # âœ… ì²­í¬ê°€ ì—¬ëŸ¬ê°œ â†’ í•©ì¹˜ê¸°
        combined_summary = " ".join(chunk_summaries)
        combined_token_count = len(kobart_tokenizer.encode(combined_summary))
        print(f"ğŸ” combined summary token count: {combined_token_count}")

        if combined_token_count <= max_input_length:
            print("âœ… ìµœì¢… ìš”ì•½ ì…ë ¥ ê¸¸ì´ ê°€ëŠ¥ â†’ ì¶”ê°€ ìš”ì•½")
            final_summary = summarize_kobart(combined_summary)
        else:
            print("âš ï¸ combined summary ê¸¸ì´ ì´ˆê³¼ â†’ ë‹¤ì‹œ ë‚˜ëˆ„ê¸°")
            new_sentences = split_text_into_sentences(combined_summary)
            new_chunks = group_sentences_by_token_limit(new_sentences, kobart_tokenizer, max_input_length)

            new_summaries = []
            for i, chunk in enumerate(new_chunks, 1):
                print(f"ğŸ”„ ì¬ë¶„í•  {i}/{len(new_chunks)} ìš”ì•½ ì¤‘...")
                summary = summarize_kobart(chunk)
                new_summaries.append(summary)
                print(f"ğŸ”„ ì¬ë¶„í•  {i}/{len(new_chunks)} summary: {summary}")

            final_combined = " ".join(new_summaries)
            final_combined_token_count = len(kobart_tokenizer.encode(final_combined))
            print(f"ğŸ” ì¬ë¶„í•  combined summary token count: {final_combined_token_count}")

            if final_combined_token_count <= max_input_length:
                print("âœ… ì¬ë¶„í• ëœ combined summary ì…ë ¥ ê°€ëŠ¥ â†’ ìµœì¢… ìš”ì•½")
                final_summary = summarize_kobart(final_combined)
            else:
                print("âš ï¸ ì¬ë¶„í• ëœ combined summaryë„ ì…ë ¥ ì´ˆê³¼ â†’ ë” ì´ìƒ ë‚˜ëˆ„ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©")
                final_summary = final_combined
    if keyword and is_relevant:
        if is_relevant(final_summary, keyword):
            return final_summary.replace('\n', ' ')
        else:
            print(f"â›”ï¸ ìœ ì‚¬ë„ ê¸°ì¤€ ë¯¸ë‹¬ â†’ ìš”ì•½ ì œì™¸")
            return None
    else:
        return final_summary.replace('\n', ' ')