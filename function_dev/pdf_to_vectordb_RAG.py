# ë‰´ìŠ¤/ë¸”ë¡œê·¸/ë…¼ë¬¸ JSON ê¸°ë°˜ ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸ (ì—ì´ì „íŠ¸ ë¶„ë¦¬)
# -----------------------------------------------------------
# ëª©ì : ë‰´ìŠ¤, ë¸”ë¡œê·¸, ë…¼ë¬¸ JSON íŒŒì¼ì„ ì½ì–´ ë²¡í„° DBì™€ chunk ë°ì´í„° ì €ì¥

import os
import json
import numpy as np
import faiss
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.vectorstores import FAISS

# ì„¤ì •ê°’
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
JSON_FILES = [
    (os.path.join(BASE_DIR, "..", "sample_news", "news_data_summaries.json"), "news"),
    (os.path.join(BASE_DIR, "..", "sample_blogs", "blogs_data_summaries.json"), "blog"),
    (os.path.join(BASE_DIR, "..", "sample_papers", "arxiv_papers_summaries.json"), "paper")
]
FAISS_INDEX_PATH = "faiss_index"
CHUNK_SAVE_PATH = "doc_chunks.npy"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# í…ìŠ¤íŠ¸ ì²­í¬í™” í•¨ìˆ˜
def chunk_text(text, chunk_size=500, chunk_overlap=50):
    splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    return splitter.split_text(text)

# JSON ë¡œë”© ë° ì²­í¬í™”
def load_chunks_from_json(file_path, source_label):
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    chunks = []
    for item in data:
        title = item.get("title", "")
        content = item.get("summary", "")
        if content:
            text = f"{title}\n{content.strip()}"
            chunked = chunk_text(text)
            chunks.extend([Document(page_content=c, metadata={"source": source_label}) for c in chunked])
    print(f"âœ… {source_label} ì²­í¬ ìˆ˜: {len(chunks)}")
    return chunks

# FAISS + ì²­í¬ ì €ì¥
def save_to_faiss_with_chunks(documents):
    model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vectordb = FAISS.from_documents(documents, embedding=model)
    vectordb.save_local(FAISS_INDEX_PATH)
    np.save(CHUNK_SAVE_PATH, np.array([{"text": d.page_content, "source": d.metadata["source"]} for d in documents]))
    print(f"ğŸ“¦ ì´ {len(documents)} chunks ì €ì¥ ì™„ë£Œ!")

# ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
if __name__ == "__main__":
    print("ğŸ“¥ JSON ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
    all_documents = []
    for file_path, label in JSON_FILES:
        chunks = load_chunks_from_json(file_path, label)
        all_documents.extend(chunks)

    if all_documents:
        save_to_faiss_with_chunks(all_documents)
    else:
        print("âš ï¸ ì €ì¥í•  chunkê°€ ì—†ìŠµë‹ˆë‹¤.")
