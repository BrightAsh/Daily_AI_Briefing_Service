# ë‰´ìŠ¤ ë° ë…¼ë¬¸ ìš”ì•½ìš© ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸ (ì—ì´ì „íŠ¸ ë¶„ë¦¬)
# -----------------------------------------------------------
# ëª©ì : ë‰´ìŠ¤ ë°ì´í„°(JSON) ë° ë…¼ë¬¸ PDFë¥¼ ì½ì–´ ë²¡í„° DBì™€ chunk ë°ì´í„° ì €ì¥

import os
import json
import glob
import fitz  # PyMuPDF
import numpy as np
import faiss
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

# ì„¤ì •ê°’
PDF_FOLDER = "../sample_papers"
NEWS_JSON_PATH = "news_data_full.json"
FAISS_INDEX_PATH = "faiss_index.index"
CHUNK_SAVE_PATH = "doc_chunks.npy"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# PDF ë¡œë”©
def load_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# í…ìŠ¤íŠ¸ ì²­í¬
def chunk_text(text, chunk_size=500, chunk_overlap=50):
    splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    return splitter.split_text(text)

# ë‰´ìŠ¤ JSON ë¡œë”© ë° ì²­í¬í™”
def load_news_chunks():
    if not os.path.exists(NEWS_JSON_PATH):
        return []
    with open(NEWS_JSON_PATH, "r", encoding="utf-8") as f:
        news_items = json.load(f)
    chunks = []
    for item in news_items:
        title = item.get("title", "")
        content = item.get("full_text", "")
        if content:
            chunks.append({
                "text": f"[ë‰´ìŠ¤] {title}\n{content.strip()}",
                "source": "news"
            })
    return chunks

# ë…¼ë¬¸ PDF ë¡œë”© ë° ì²­í¬í™”
def load_all_pdfs(pdf_folder):
    all_chunks = []
    model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    for file_path in glob.glob(os.path.join(pdf_folder, "*.pdf")):
        print(f"ğŸ“„ Processing: {file_path}")
        text = load_pdf(file_path)
        chunks = chunk_text(text)
        named_chunks = [
            {"text": chunk, "source": "paper"} for chunk in chunks
        ]
        all_chunks.extend(named_chunks)
    return all_chunks

# FAISS + ì²­í¬ ì €ì¥
def save_to_faiss_with_chunks(named_chunks):
    texts = [chunk["text"] for chunk in named_chunks]
    model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    embeddings = model.embed_documents(texts)
    dimension = len(embeddings[0])
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    faiss.write_index(index, FAISS_INDEX_PATH)
    np.save(CHUNK_SAVE_PATH, np.array(named_chunks))
    print(f"âœ… ì´ {len(named_chunks)} chunks ì €ì¥ ì™„ë£Œ! (ë‰´ìŠ¤ + ë…¼ë¬¸ í¬í•¨)")

# ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
if __name__ == "__main__":
    base_path = os.path.dirname(os.path.abspath(__file__))
    full_pdf_path = os.path.join(base_path, PDF_FOLDER)

    print("ğŸ“¥ ë‰´ìŠ¤ + ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘...")
    news_chunks = load_news_chunks()
    print(f"ğŸ“° ë‰´ìŠ¤ ì²­í¬ ìˆ˜: {len(news_chunks)}")
    if news_chunks:
        print(f"ì˜ˆì‹œ ë‰´ìŠ¤ ì œëª©: {news_chunks[0]['text'][:100]}...")

    if os.path.exists(full_pdf_path):
        paper_chunks = load_all_pdfs(full_pdf_path)
    else:
        print(f"âŒ PDF í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {full_pdf_path}")
        paper_chunks = []

    all_chunks = news_chunks + paper_chunks
    if all_chunks:
        save_to_faiss_with_chunks(all_chunks)
    else:
        print("âš ï¸ ì €ì¥í•  chunkê°€ ì—†ìŠµë‹ˆë‹¤.")
