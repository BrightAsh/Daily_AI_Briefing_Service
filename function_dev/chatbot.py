# OpenAI + FAISS Í∏∞Î∞ò Q&A Ï±óÎ¥á (Gradio + MMR + OpenAI ÏµúÏã† API)
# -----------------------------------------------------------

import numpy as np
import faiss
import os
from sentence_transformers import SentenceTransformer
import gradio as gr
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI

# --- ÏÑ§Ï†ï ---
EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"
FAISS_INDEX_PATH = "./faiss_index.index"
DOCUMENTS_PATH = "./doc_chunks.npy"
OPENAI_MODEL = "gpt-3.5-turbo"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# OpenAI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
client = OpenAI(api_key=OPENAI_API_KEY)

# --- Î™®Îç∏ Î°úÎìú ---
embedder = SentenceTransformer(EMBEDDING_MODEL)
index = faiss.read_index(FAISS_INDEX_PATH)
chunks = np.load(DOCUMENTS_PATH, allow_pickle=True)

# --- FAISS Ï†ÑÏ≤¥ Î≤°ÌÑ∞ Ï∂îÏ∂ú (MMRÏö©) ---
def get_all_vectors():
    return np.array([index.reconstruct(i) for i in range(index.ntotal)])

# --- MMR ÏïåÍ≥†Î¶¨Ï¶ò ---
def mmr(query_vec, doc_vecs, lambda_param=0.5, top_k=5):
    sim_to_query = cosine_similarity(query_vec, doc_vecs)[0]
    selected = []
    selected_set = set()
    while len(selected) < top_k:
        mmr_score = []
        for i in range(len(doc_vecs)):
            if i in selected_set:
                mmr_score.append(-np.inf)
                continue
            if not selected:
                diversity_penalty = 0
            else:
                sim_to_selected = max(cosine_similarity([doc_vecs[i]], [doc_vecs[j] for j in selected])[0])
                diversity_penalty = sim_to_selected
            score = lambda_param * sim_to_query[i] - (1 - lambda_param) * diversity_penalty
            mmr_score.append(score)
        idx = np.argmax(mmr_score)
        selected.append(idx)
        selected_set.add(idx)
    return selected

# --- Q&A Ìï®Ïàò ---
def answer_question(query, top_k=5):
    query_vec = embedder.encode([query])
    doc_vecs = get_all_vectors()
    mmr_indices = mmr(query_vec, doc_vecs, lambda_param=0.5, top_k=top_k)
    context = "\n\n".join([chunks[i] for i in mmr_indices])

    prompt = f"""
Îã§Ïùå Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ï†ïÌôïÌïòÍ≥† Í∞ÑÍ≤∞ÌïòÍ≤å ÎãµÎ≥ÄÌï¥Ï§ò. Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÄ Ï∂îÏ∏°ÌïòÏßÄ Îßà.

Î¨∏ÏÑú:
{context}

ÏßàÎ¨∏: {query}
ÎãµÎ≥Ä:
"""

    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": "ÎãπÏã†ÏùÄ Î¨∏ÏÑúÎ•º Í∏∞Î∞òÏúºÎ°ú Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÎäî AI ÎπÑÏÑúÏûÖÎãàÎã§."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=500
    )
    return response.choices[0].message.content

# --- Gradio UI Íµ¨ÏÑ± ---
def gradio_interface(query):
    return answer_question(query)

iface = gr.Interface(
    fn=gradio_interface,
    inputs=gr.Textbox(lines=2, placeholder="ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî..."),
    outputs="text",
    title="üìö OpenAI RAG Ï±óÎ¥á (MMR Í∏∞Î∞ò)",
    description="Î¨∏ÏÑúÎ•º Í∏∞Î∞òÏúºÎ°ú GPTÍ∞Ä ÎãµÎ≥ÄÌï¥Ï£ºÎäî Ï±óÎ¥áÏûÖÎãàÎã§. MMR Î∞©ÏãùÏúºÎ°ú chunk Í≤ÄÏÉâÏùÑ ÏàòÌñâÌï©ÎãàÎã§."
)

if __name__ == "__main__":
    iface.launch()
