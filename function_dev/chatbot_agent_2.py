# Agent ê¸°ë°˜ ë¦¬íŒ©í„°ë§: LangChain + FAISS + Tool í˜¸ì¶œ ìë™í™” + MCP Tool ì¶”ê°€
# ---------------------------------------------------------------------

import os
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.agents import Tool, initialize_agent, AgentType
from langchain.chat_models import ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

# ë²¡í„° DB ë¡œë”©
CHUNK_PATH = "doc_chunks.npy"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
chunks = np.load(CHUNK_PATH, allow_pickle=True)
embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
documents = [Document(page_content=c["text"]) for c in chunks if isinstance(c, dict)]
vectordb = FAISS.from_documents(documents, embedding=embedding_model)
retriever = vectordb.as_retriever(search_type="mmr", search_kwargs={"k": 3})

# ì§ˆì˜ì‘ë‹µ ì²´ì¸ ìƒì„± (ë¬¸ì„œ ê¸°ë°˜)
rag_chain = RetrievalQA.from_chain_type(
    retriever=retriever,
    llm=ChatOpenAI(model_name="gpt-4", temperature=0.3),
    return_source_documents=False
)

# ì•ˆì „í•œ RAG ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def safe_rag_run(query: str) -> str:
    docs = retriever.get_relevant_documents(query)
    joined = " ".join([doc.page_content for doc in docs])
    if not joined or len(joined.strip()) < 50:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    return rag_chain.run(query)

# MCP ê¸°ë°˜ ìš”ì•½ê¸° í•¨ìˆ˜ ì •ì˜
def mcp_summarize_tool(text: str) -> str:
    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ì…ë ¥ëœ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê±°ë‚˜ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì•¼."},
        {"role": "user", "content": f"{text}"}
    ]
    response = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        temperature=0.2
    )
    return response.choices[0].message.content

# íˆ´ ì •ì˜
qa_tool = Tool(
    name="document_query_tool",
    func=safe_rag_run,
    description="ì²˜ìŒ ë“¤ì–´ë³´ëŠ” ê°œë…ì— ëŒ€í•œ ì§ˆë¬¸ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
)

mcp_tool = Tool(
    name="text_summarizer",
    func=mcp_summarize_tool,
    description="ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìš”ì•½ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì í•©í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì§ˆë¬¸ì´ ë‹¨ìˆœí•˜ê±°ë‚˜ ë¬¸ë§¥ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ì‚¬ìš©í•˜ì„¸ìš”."
)

# Agent ì´ˆê¸°í™”
agent = initialize_agent(
    tools=[qa_tool, mcp_tool],
    llm=ChatOpenAI(model_name="gpt-4", temperature=0.3),
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# ì‚¬ìš©ì ì…ë ¥
query = input("ğŸ¤– ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# ì‹¤í–‰
response = agent.run(query)
print("\nğŸ§  ë‹µë³€:")
print(response)
