import json
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

# 1ï¸âƒ£ KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')

# 2ï¸âƒ£ íŒŒì¼ ê²½ë¡œ
INPUT_FILE = "news_data_full.json"
OUTPUT_FILE = "news_data_summaries.json"

# 3ï¸âƒ£ ìš”ì•½ í•¨ìˆ˜ (ë°˜ë³µ ì–µì œ íŒŒë¼ë¯¸í„° ì ìš©)
def summarize_kobart(text, max_input_length=1024):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=max_input_length, truncation=True)
    summary_ids = model.generate(
        inputs,
        max_length=200,
        min_length=30,
        num_beams=4,
        no_repeat_ngram_size=3,   # ğŸ”¥ 3ê·¸ë¨ ë°˜ë³µ ë°©ì§€
        repetition_penalty=2.0,   # ğŸ”¥ ë°˜ë³µ ì–µì œ ê°•í™”
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# 4ï¸âƒ£ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ ì¶”ì¶œ í•¨ìˆ˜
def extract_sentences_with_keywords(text, keywords):
    sentences = text.split('.')
    selected = [s for s in sentences if any(kw in s for kw in keywords)]
    return '. '.join(selected)

# 5ï¸âƒ£ ì¤‘ë³µ ë¬¸ì¥ ì œê±° í•¨ìˆ˜
def remove_duplicate_sentences(text):
    seen = set()
    result = []
    sentences = text.split('.')
    for s in sentences:
        s_clean = s.strip()
        if s_clean and s_clean not in seen:
            seen.add(s_clean)
            result.append(s_clean)
    return '. '.join(result)

# 6ï¸âƒ£ ê³„ì¸µì  ìš”ì•½ í•¨ìˆ˜ (ë¶„í•  + ë¶€ë¶„ ìš”ì•½ + ìµœì¢… ìš”ì•½ + ì¤‘ë³µ ì œê±°)
def hierarchical_summary(full_text, keywords=None, chunk_size=1000):
    # í‚¤ì›Œë“œ í•„í„°ë§ (ì„ íƒ)
    if keywords:
        print(f"ğŸ” í‚¤ì›Œë“œ ì¤‘ì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¤‘... í‚¤ì›Œë“œ: {keywords}")
        filtered_text = extract_sentences_with_keywords(full_text, keywords)
        if filtered_text.strip():
            full_text = filtered_text
        else:
            print("âš ï¸ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ì´ ì—†ì–´ ì „ì²´ ë³¸ë¬¸ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    # ë¶„í•  ìš”ì•½
    text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    chunk_summaries = []
    for i, chunk in enumerate(text_chunks, 1):
        print(f"    ğŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} ìš”ì•½ ì¤‘...")
        summary = summarize_kobart(chunk)
        chunk_summaries.append(summary)

    # ë¶€ë¶„ ìš”ì•½ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summary = " ".join(chunk_summaries)
    print("    ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
    final_summary = summarize_kobart(combined_summary)

    # ìµœì¢… ìš”ì•½ í›„ ì¤‘ë³µ ì œê±°
    cleaned_summary = remove_duplicate_sentences(final_summary)
    return cleaned_summary

# 7ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
with open(INPUT_FILE, "r", encoding="utf-8") as f:
    articles = json.load(f)

summarized_articles = []

# 8ï¸âƒ£ ìš”ì•½ ì‹¤í–‰ (í‚¤ì›Œë“œ ì¤‘ì‹¬ ìš”ì•½)
# ğŸ‘‰ í‚¤ì›Œë“œ ì—†ìœ¼ë©´ None ë˜ëŠ” []ë¡œ ì…ë ¥
KEYWORDS = ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë”¥ëŸ¬ë‹']  # í•„ìš”ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥

for idx, article in enumerate(articles, 1):
    title = article.get("title", "")
    full_text = article.get("full_text", "").strip()

    if not full_text:
        print(f"\n[{idx}] âš ï¸ {title}: ë³¸ë¬¸ ì—†ìŒ (ìŠ¤í‚µ)")
        continue

    print(f"\n[{idx}] ğŸ“° {title}")
    print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(full_text)}ì")

    try:
        summary = hierarchical_summary(full_text, keywords=KEYWORDS)
        print(f"âœ… ìµœì¢… ìš”ì•½ ì™„ë£Œ:\n{summary}")

        summarized_articles.append({
            "title": title,
            "url": article.get("url"),
            "source": article.get("source"),
            "publishedAt": article.get("publishedAt"),
            "summary": summary
        })
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")

# 9ï¸âƒ£ ìš”ì•½ ê²°ê³¼ ì €ì¥
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(summarized_articles, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ì´ {len(summarized_articles)}ê±´ ìš”ì•½ ì™„ë£Œ â†’ '{OUTPUT_FILE}'ì— ì €ì¥ë¨!")
