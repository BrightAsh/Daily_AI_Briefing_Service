import json
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

# 1ï¸âƒ£ KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')

# 2ï¸âƒ£ íŒŒì¼ ê²½ë¡œ
INPUT_FILE = "blogs_data.json"
OUTPUT_FILE = "blogs_data_summaries.json"

# 3ï¸âƒ£ ìš”ì•½ í•¨ìˆ˜ (ë°˜ë³µ ì–µì œ íŒŒë¼ë¯¸í„° ì ìš©)
def summarize_kobart(text, max_input_length=1024):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=max_input_length, truncation=True)
    summary_ids = model.generate(
        inputs,
        max_length=200,
        min_length=30,
        num_beams=4,
        no_repeat_ngram_size=3,   # ğŸ”¥ 3ê·¸ë¨ ë°˜ë³µ ë°©ì§€
        repetition_penalty=2.0,   # ğŸ”¥ ë°˜ë³µ ì–µì œ ê°•í™”
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# 4ï¸âƒ£ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ ì¶”ì¶œ í•¨ìˆ˜
def extract_sentences_with_keywords(text, keywords):
    sentences = text.split('.')
    selected = [s for s in sentences if any(kw in s for kw in keywords)]
    return '. '.join(selected)

# 5ï¸âƒ£ ì¤‘ë³µ ë¬¸ì¥ ì œê±° í•¨ìˆ˜
def remove_duplicate_sentences(text):
    seen = set()
    result = []
    sentences = text.split('.')
    for s in sentences:
        s_clean = s.strip()
        if s_clean and s_clean not in seen:
            seen.add(s_clean)
            result.append(s_clean)
    return '. '.join(result)

# 6ï¸âƒ£ ê³„ì¸µì  ìš”ì•½ í•¨ìˆ˜ (ë¶„í•  + ë¶€ë¶„ ìš”ì•½ + ìµœì¢… ìš”ì•½ + ì¤‘ë³µ ì œê±°)
def hierarchical_summary(full_text, keywords=None, chunk_size=1000):
    # í‚¤ì›Œë“œ í•„í„°ë§ (ì„ íƒ)
    if keywords:
        print(f"ğŸ” í‚¤ì›Œë“œ ì¤‘ì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¤‘... í‚¤ì›Œë“œ: {keywords}")
        filtered_text = extract_sentences_with_keywords(full_text, keywords)
        if filtered_text.strip():
            full_text = filtered_text
        else:
            print("âš ï¸ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ì´ ì—†ì–´ ì „ì²´ ë³¸ë¬¸ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    # ë¶„í•  ìš”ì•½
    text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    chunk_summaries = []
    for i, chunk in enumerate(text_chunks, 1):
        print(f"    ğŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} ìš”ì•½ ì¤‘...")
        summary = summarize_kobart(chunk)
        chunk_summaries.append(summary)

    # ë¶€ë¶„ ìš”ì•½ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summary = " ".join(chunk_summaries)
    print("    ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
    final_summary = summarize_kobart(combined_summary)

    # ìµœì¢… ìš”ì•½ í›„ ì¤‘ë³µ ì œê±°
    cleaned_summary = remove_duplicate_sentences(final_summary)
    return cleaned_summary

