import json
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
from sentence_transformers import SentenceTransformer, util

# ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# 1ï¸âƒ£ KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')

# 2ï¸âƒ£ íŒŒì¼ ê²½ë¡œ
INPUT_FILE = "blogs_data.json"
OUTPUT_FILE = "blogs_data_summaries.json"

# 3ï¸âƒ£ ìš”ì•½ í•¨ìˆ˜ (ë°˜ë³µ ì–µì œ íŒŒë¼ë¯¸í„° ì ìš©)
def summarize_kobart(text, max_input_length=1024):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=max_input_length, truncation=True)
    summary_ids = model.generate(
        inputs,
        max_length=200,
        min_length=30,
        num_beams=4,
        no_repeat_ngram_size=3,   # ðŸ”¥ 3ê·¸ëž¨ ë°˜ë³µ ë°©ì§€
        repetition_penalty=2.0,   # ðŸ”¥ ë°˜ë³µ ì–µì œ ê°•í™”
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# 4ï¸âƒ£ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ìž¥ ì¶”ì¶œ í•¨ìˆ˜
def extract_sentences_with_keywords(text, keywords):
    sentences = text.split('.')
    selected = [s for s in sentences if any(kw in s for kw in keywords)]
    return '. '.join(selected)

# 5ï¸âƒ£ ì¤‘ë³µ ë¬¸ìž¥ ì œê±° í•¨ìˆ˜
def remove_duplicate_sentences(text):
    seen = set()
    result = []
    sentences = text.split('.')
    for s in sentences:
        s_clean = s.strip()
        if s_clean and s_clean not in seen:
            seen.add(s_clean)
            result.append(s_clean)
    return '. '.join(result)

# 6ï¸âƒ£ ê³„ì¸µì  ìš”ì•½ í•¨ìˆ˜ (ë¶„í•  + ë¶€ë¶„ ìš”ì•½ + ìµœì¢… ìš”ì•½ + ì¤‘ë³µ ì œê±°)
def hierarchical_summary(full_text, keyword=None, chunk_size=1000):
    # í‚¤ì›Œë“œ í•„í„°ë§ (ì„ íƒ)
    # if keywords:
    #     print(f"ðŸ”Ž í‚¤ì›Œë“œ ì¤‘ì‹¬ ë¬¸ìž¥ ì¶”ì¶œ ì¤‘... í‚¤ì›Œë“œ: {keywords}")
    #     filtered_text = extract_sentences_with_keywords(full_text, keywords)
    #     if filtered_text.strip():
    #         full_text = filtered_text
    #     else:
    #         print("âš ï¸ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ìž¥ì´ ì—†ì–´ ì „ì²´ ë³¸ë¬¸ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    # ë¶„í•  ìš”ì•½
    text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    chunk_summaries = []
    for i, chunk in enumerate(text_chunks, 1):
        print(f"    ðŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} ìš”ì•½ ì¤‘...")
        summary = summarize_kobart(chunk)
        chunk_summaries.append(summary)

    # ë¶€ë¶„ ìš”ì•½ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summary = " ".join(chunk_summaries)
    print("    ðŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
    final_summary = summarize_kobart(combined_summary)

    if not is_relevant(final_summary, keyword):
        print(f"â›”ï¸ ë¬´ê´€í•œ ë¸”ë¡œê·¸ ì œì™¸: \"{final_summary[:100]}...\"")
        return None

    # ìµœì¢… ìš”ì•½ í›„ ì¤‘ë³µ ì œê±°
    cleaned_summary = remove_duplicate_sentences(final_summary)
    return cleaned_summary

def is_relevant(summary: str, keyword: str) -> bool:
    # ìœ ì‚¬ë„ í•„í„° ê¸°ì¤€ (0~1 ì‚¬ì´ ê°’, ë†’ì„ìˆ˜ë¡ ì—„ê²©í•¨)
    SIMILARITY_THRESHOLD = 0.2

    embeddings = embedder.encode([summary, keyword])
    similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
    print(f"    ðŸ” ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.4f} (í‚¤ì›Œë“œ: {keyword})")
    return similarity >= SIMILARITY_THRESHOLD