import json
# from transformers import LEDTokenizer, LEDForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartTokenizer, BartForConditionalGeneration

# 1ï¸âƒ£ LED ìš”ì•½ ëª¨ë¸ ë¡œë“œ
# model_name = 'allenai/led-base-16384'
# tokenizer = LEDTokenizer.from_pretrained(model_name)
# model = LEDForConditionalGeneration.from_pretrained(model_name)

# # 2ï¸âƒ£ ìš”ì•½ í•¨ìˆ˜ (LED)
# def summarize_led(text, max_input_length=16000):
#     inputs = tokenizer.encode(text, return_tensors="pt", max_length=max_input_length, truncation=True)
#     attention_mask = inputs.ne(tokenizer.pad_token_id).long()  # LEDëŠ” attention_mask í•„ìš”
#     summary_ids = model.generate(
#         inputs,
#         attention_mask=attention_mask,
#         max_length=512,      # ì¶œë ¥ ê¸¸ì´
#         num_beams=4,
#         early_stopping=True
#     )
#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
#     return summary

# # 3ï¸âƒ£ ì¤‘ë³µ ë¬¸ì¥ ì œê±° í•¨ìˆ˜ (ë™ì¼)
# def remove_duplicate_sentences(text):
#     seen = set()
#     result = []
#     sentences = text.split('.')
#     for s in sentences:
#         s_clean = s.strip()
#         if s_clean and s_clean not in seen:
#             seen.add(s_clean)
#             result.append(s_clean)
#     return '. '.join(result)

# 4ï¸âƒ£ ë¸”ë¡œê·¸ ë²ˆì—­ ëª¨ë¸
translation_model_name = "seongs/ke-t5-base-aihub-koen-translation-integrated-10m-en-to-ko"
translation_tokenizer = AutoTokenizer.from_pretrained(translation_model_name)
translation_model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)

# 5ï¸âƒ£ ë¸”ë¡œê·¸ ë²ˆì—­ í•¨ìˆ˜
def translate_summaries(text):
    print(text)
    inputs = translation_tokenizer([text], return_tensors="pt", padding=True, truncation=True)
    outputs = translation_model.generate(**inputs, max_length=2048)
    translated_text = translation_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translated_text

# # 6ï¸âƒ£ ê³„ì¸µì  ìš”ì•½ í•¨ìˆ˜ (ì˜ì–´ ë…¼ë¬¸ ì „ìš©)
# def hierarchical_summary_led(full_text, chunk_size=12000):
#     # ë¶„í•  ìš”ì•½
#     text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
#     chunk_summaries = []
#     for i, chunk in enumerate(text_chunks, 1):
#         print(f"    ğŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} ìš”ì•½ ì¤‘...")
#         summary = summarize_led(chunk)
#         chunk_summaries.append(summary)

#     # ë¶€ë¶„ ìš”ì•½ í•©ì³ì„œ ìµœì¢… ìš”ì•½
#     combined_summary = " ".join(chunk_summaries)
#     print("    ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
#     final_summary = summarize_led(combined_summary)

#     # ìµœì¢… ìš”ì•½ í›„ ì¤‘ë³µ ì œê±°
#     cleaned_summary = remove_duplicate_sentences(final_summary)

#     translated_summary = translate_summaries(cleaned_summary)

#     return translated_summary

tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

def summarize_bart(text):
    inputs = tokenizer([text], return_tensors="pt", max_length=1536, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)
    final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    # # ìµœì¢… ìš”ì•½ í›„ ì¤‘ë³µ ì œê±°
    # cleaned_summary = remove_duplicate_sentences(final_summary)

    translated_summary = translate_summaries(final_summary)

    return translated_summary