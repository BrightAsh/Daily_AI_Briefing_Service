import json
from transformers import LEDTokenizer, LEDForConditionalGeneration

# 1ï¸âƒ£ LED ìš”ì•½ ëª¨ë¸ ë¡œë“œ
model_name = 'allenai/led-base-16384'
tokenizer = LEDTokenizer.from_pretrained(model_name)
model = LEDForConditionalGeneration.from_pretrained(model_name)

# 2ï¸âƒ£ ìš”ì•½ í•¨ìˆ˜ (LED)
def summarize_led(text, max_input_length=16000):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=max_input_length, truncation=True)
    attention_mask = inputs.ne(tokenizer.pad_token_id).long()  # LEDëŠ” attention_mask í•„ìš”
    summary_ids = model.generate(
        inputs,
        attention_mask=attention_mask,
        max_length=512,      # ì¶œë ¥ ê¸¸ì´
        num_beams=4,
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# 3ï¸âƒ£ ì¤‘ë³µ ë¬¸ì¥ ì œê±° í•¨ìˆ˜ (ë™ì¼)
def remove_duplicate_sentences(text):
    seen = set()
    result = []
    sentences = text.split('.')
    for s in sentences:
        s_clean = s.strip()
        if s_clean and s_clean not in seen:
            seen.add(s_clean)
            result.append(s_clean)
    return '. '.join(result)

# 4ï¸âƒ£ ê³„ì¸µì  ìš”ì•½ í•¨ìˆ˜ (ì˜ì–´ ë…¼ë¬¸ ì „ìš©)
def hierarchical_summary_led(full_text, chunk_size=12000):
    # ë¶„í•  ìš”ì•½
    text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    chunk_summaries = []
    for i, chunk in enumerate(text_chunks, 1):
        print(f"    ğŸ§© ë¶€ë¶„ {i}/{len(text_chunks)} ìš”ì•½ ì¤‘...")
        summary = summarize_led(chunk)
        chunk_summaries.append(summary)

    # ë¶€ë¶„ ìš”ì•½ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summary = " ".join(chunk_summaries)
    print("    ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
    final_summary = summarize_led(combined_summary)

    # ìµœì¢… ìš”ì•½ í›„ ì¤‘ë³µ ì œê±°
    cleaned_summary = remove_duplicate_sentences(final_summary)
    return cleaned_summary