import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartTokenizer, BartForConditionalGeneration
from sentence_transformers import SentenceTransformer, util

# ë¸”ë¡œê·¸ ë²ˆì—­ ëª¨ë¸
translation_model_name = "seongs/ke-t5-base-aihub-koen-translation-integrated-10m-en-to-ko"
translation_tokenizer = AutoTokenizer.from_pretrained(translation_model_name)
translation_model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)

# ë¸”ë¡œê·¸ ë²ˆì—­ í•¨ìˆ˜
def translate_summaries(text):
    print(text)
    inputs = translation_tokenizer([text], return_tensors="pt", padding=True, truncation=True)
    outputs = translation_model.generate(**inputs, max_length=2048)
    translated_text = translation_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translated_text

# âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# âœ… ìœ ì‚¬ë„ í•„í„° í•¨ìˆ˜
def is_relevant(summary: str, keyword: str, threshold=0.1) -> bool:
    embeddings = embedder.encode([summary, keyword])
    similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
    print(f"    ğŸ” ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.4f} (í‚¤ì›Œë“œ: {keyword})")
    return similarity >= threshold

# BART ìš”ì•½ ëª¨ë¸
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# âœ… ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
def split_text_into_sentences(text):
    import re
    return re.split(r'(?<=[.!?])\s+', text)

# âœ… ë¬¸ì¥ í† í° ê¸°ì¤€ ê·¸ë£¹í™”
def group_sentences_by_token_limit(sentences, tokenizer, max_tokens):
    groups = []
    current_group = ""
    for sentence in sentences:
        tentative_group = current_group + " " + sentence if current_group else sentence
        tokenized = tokenizer.encode(tentative_group, return_tensors="pt")
        if tokenized.size(1) <= max_tokens:
            current_group = tentative_group
        else:
            if current_group:
                groups.append(current_group.strip())
            current_group = sentence
    if current_group:
        groups.append(current_group.strip())
    return groups

# ìš”ì•½ í•¨ìˆ˜
def summarize_bart(text, keyword):
    inputs = tokenizer([text], return_tensors="pt", max_length=2048, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=1024, num_beams=4, early_stopping=True)
    final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    if keyword and is_relevant:
        if is_relevant(final_summary, keyword):
            final_summary = final_summary.replace('\n', ' ')
        else:
            print(f"â›”ï¸ ìœ ì‚¬ë„ ê¸°ì¤€ ë¯¸ë‹¬ â†’ ìš”ì•½ ì œì™¸")
            return None
    else:
        final_summary = final_summary.replace('\n', ' ')

    # ìš”ì•½ë³¸ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
    sentences = split_text_into_sentences(final_summary)
    
    # ë¶„í• ëœ ë¬¸ì¥ ê°ê° ë²ˆì—­
    translated_sentences = [translate_summaries(sentence) for sentence in sentences]

    # ìµœì¢… ìš”ì•½ë³¸ ìƒì„±
    final_summary = " ".join(translated_sentences)

    return final_summary